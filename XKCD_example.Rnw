\documentclass{article}
\usepackage[margin=1.25in]{geometry}
%\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}

\begin{document}


<<concordance, echo=FALSE>>=
opts_knit$set(self.contained=FALSE)

library(lda)
library(tm)
library(stm)
library(topicmodels)
library(Matrix)

library(textir)
@

using XKCD data as an example, transcriptions from XX years worth of comics. Binned individual comic strips into 11 groups based on order in which they were published. (picked 11 because it divides sans remainder.) Although artificial, treating each bin as a "group" to mimic coveriates, as an analogue to the race, gender, party affiliation, etc, we expect to accompny our open ended text in the ANES data.


<< first-chunk,echo=FALSE >>=
set.seed(11123)

#preprocessing stuff: convert to lower case, stemming, remove stop words and numbers and punctuation, 
dfTranscripts <- read.table("transcriptsB.csv",header=F,sep="\t",colClasses=c("character","character"),col.names=c("url","text"),quote="")
dfTranscripts$number<- seq(from=1,to=1265,by=1)
dfTranscripts$group<- rep(1:11,each=115)
dfTranscripts$group <- as.factor(dfTranscripts$group)
xkcd <- textProcessor(metadata = dfTranscripts, documents=dfTranscripts$text)



Xmeta<-xkcd$meta
Xvocab<-xkcd$vocab
Xdocs<-xkcd$documents

Xout <- prepDocuments(Xdocs, Xvocab, Xmeta)

docsX<-Xout$documents
vocabX<-Xout$vocab
metaX <-Xout$meta

@

need to describe the Variational expectation maximization (EM) algorithms used by STM package. (fast variant of nonconjugate variational Expectation Maximization
Roberts et al use this rather than collapsed Gibbs Sampling or mean field variational Gibbs because of nonconjugacy due to the choice of Logistic Normal for the prior.

<<models,echo=FALSE >>=
#k is the number of topics to fit.

#estimate a single model
#prevalence is the covariates
xkcdPrevFit10 <- stm(docsX,vocabX,K=10, prevalence=~metaX$group)

#fit multiple modles to compare graphically
#runs is the number of separate models and (random) starting points
xkcdSelect10 <- selectModel(docsX,vocabX, K=10, runs=10)

#view lists of words in each topic
labelTopics(xkcdPrevFit10)
@

\end{document}
