\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}

\setlength{\parskip}{.5em}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\newcommand{\jp}[1]{{\color{blue}#1}}

\newcommand{\R}{\mathbb{R}}

\title{What They're Talking About\\
\vspace*{1\baselineskip}
\large Applying the Structural Topic Model to Open Ended Survey Responses \\
from the American National Election Studies}

\author{Joseph Eduardo Papio\\
\vspace*{1\baselineskip}
\small Iowa State University}

\begin{document}

\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE, cache=TRUE, width=40)
library(tidyverse)
library(quanteda)
library(RColorBrewer)
library(stm)
library(ggrepel)
library(forcats)

# save(feelC.FX, scree2, partyQ.clouds1,  partyQ.clouds2, partyQc.FX, feel.clouds1,feel.clouds2, feel.FREX, year.FREX, file="objsForPaper.Rdata")
# save(year.FX, party.FX, race.int2.FX, gen.int2.FX, edu.int2.FX, inc.int2.FX, ide.int2.FX, chu.int2.FX, file="FX.Rdata")
load(file="objsForPaper.Rdata")
load(file="FX.Rdata")

str_break = function(x, width = 70L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}

normy <- function(x){
  x %>% mutate(
    normed = props/sum(props)) %>% 
    group_by(tops) %>%
    summarize(total = sum(normed)) %>%
    arrange(desc(total)) %>% mutate(
      ord = 1:length(total)
    )
}

chop1 <- function(x, n=50){
  x %>% arrange(desc(proportion)) %>% slice(1:n)
}

example1 <-"He has turned his back on the coal and steel industry in favor of importing from foreign countries.  He has West Virginia, Ohio, and Pennsylvania on its knees."

example2 <- dfm(example1, stem=TRUE, removeNumbers=TRUE, 
                 removePunct=TRUE, removeSeparators=TRUE,
                 ignoredFeatures=c(stopwords("english"), 
                                   "na", "ao", "wm", "tm", "ae", "ge"))

example3 <- "I think my husband's disability and our social security is in danger.  He will cut this and the disability is my husband's only income.  He has already cut some out of our check."

example4 <- dfm(gsub("social security", "socialsecurity", example3), stem=TRUE, 
                removeNumbers=TRUE, removePunct=TRUE, removeSeparators=TRUE, 
                ignoredFeatures=c(stopwords("english"), 
                                   "na", "ao", "wm", "tm", "ae", "ge"))
@

\section{Introduction, Motivation, and Literature Review}

\subsection{Open Ended versus Traditional Surveys}
 
Open ended survey questions provide an opportunity to solicit unguided opinions and ideas from participants, avoiding possible biases that can be introduced when utilizing more traditional survey forms in which respondents are prompted to select their answer from one of several predetermined options in a closed set of choices. 

The immediate benefit of such traditional survey formats is that once the data are collected, the analysis, including estimation of the prevalence of those opinions within the population, is fairly straight forward, and generally requires only relatively low amounts of personnel power and computational resources. However, by prompting respondents to select from among these predetermined choices, which must be decided upon by researchers prior to implementation, ideas which are already present in the political collective consciousness but not yet salient to researchers may be papered over and thus lost. Considering such information loss, exploration and application of of open ended (and thus ``unprimed") survey responses is an area of great interest and potential in the humanities and social sciences.

Unlike analysis of more traditionally formatted surveys, open ended surveys can be much more resource intensive to analyze. Prior to analysis, the data often require large amounts of in person processing to read and code the responses. Readers must be trained before they can perform the task of reading through individual responses and then use their judgement to decide what topic or topics each open ended response is about, often selecting those topics from options generated by the researchers. Thus, in addition to the large resource requirements, some topical information contained in the responses may still go still go unrecognized when utilizing this method of analyzing open ended surveys if that topical information didnâ€™t fit neatly into any of the options predetermined by researchers or readers.

However, as computing power increases, interest in the development, exploration, and application of automated textual analysis methods such as topic models  continues to grow, especially for humanities and social science researchers. Such interest is likely to be stronger with researchers who are required to first build survey instruments before interacting with respondents, as the time required to construct the instruments prior to deployment can introduce significant lag. During such time, nascent ideas forming within the population may be missed or detected by other researchers.

%%in cutting edge areas of research, like election analysis, such applications could provided scholars with insights almost in ``real time", potentially just days after the data are collected.

\subsection{pLSI, LDA, and STM}

Topic models are a subfield of Machine Learning and Natural Language Processing. Collectively they represent an effort to bring some level of automation, speed, and reliability to quantifying the latent thematic information present in text responses. Such efforts can be applied to texts of varying sizes, including the open ended surveys. Thus, in the past two decades of their development, they have great promise for reducing the resource burdens mentioned previously.

Probabilistic Latent Semantic Indexing (pLSI) was one of the earliest forms of topic modeling to be proposed  \citep{hofmann:1999}. pLSI is derived from latent class modeling for factor analysis of count data. It relies on associating latent class variables, I.E. topics, with how observed data (words) co-occur. Despite many limitations, pLSI was a pioneering step in the development of the foundation for the Topic Models framework. 

Latent Dirichlet Allocation (LDA) \citep{LDA2003} built on the work of pLSI. Blei, Ng and Jordan assume both documents within the corpus and words within the document are exchangeable, which is to say the order does not matter, and any word/document can come before or after any other word/document. This treatment of words within a document is referred to as the ``bag of words" approach.  Although this approach with respect to the words within a document causes some loss of syntactic information, particularly in a language like English where the word ordering often conveys meaning about the grammatical relationships between words, this is not a major concern for topic modeling, since the primary focus is on ascertaining the semantic and thematic content of the set of documents in question. 

%%  A Dirichlet distribution with alpha params $\leq 1$ pushes the generated multinomials towards an ``edge" of the simplex such that unlike the ``center" of the Dirichlet, probability is redistributed away from some topics and moved to other topics.  This is different from the assumptions made (or rather, not made) by pLSI, where all topics have some non negligible probablility for each document.

Blei further builds on LDA, addressing the somewhat unrealistic expectation with respect to large collections of documents that there is no correlation among topics or documents, with the introduction of Correlated Topic models \citep{Blei:2007jy}. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that were published previously. Thus, correlated topic models build on LDA by relaxing assumption that documents are independent and identically distributed.

Further developments in topic modeling include the Dirichlet-multinomial Regression model to allow covariates to affect topic prevalence \citep{TMCAFDMR} and change the rate at which a topic appears in a given document and allowing topic content to vary in log-frequency from a the background/mean \citep{SAGM}, thus allowing respondents to discuss a given topic at the same rate but in different ways (I.E. using different words).

Taking these developments and combining them, Structural Topic Models (STM) \citep{stm2013} offer a general framework to allow the inclusion of document level covariates which can influence either the rate at which a topic appears in a document \textit{or} what words are more or less likely to be used when a given topic is discussed by different groups. In the open ended surveys analyzed here, the covariates explored include the year of the election, demographic information about the respondent (race, gender, income, education, ideology, church attendance and party affiliation), the candidate's political party, and whether the thing the respondent mentions in their response is likely to make them vote \textit{for} (like) or \textit{against} (dislike) that candidate.

\citet{stm2013} apply their method of Structural Topic Modeling to open ended survey data from the American National Election Studies (ANES) collected in 2008 in which respondents were asked to identify the two most important political problems facing the United States and personal issues in the election. 


\section{Methodology}

\subsection{Definitions}
The structural topic model, like LDA and other topic models, assumes an underlying process involving unobserved semantic units, which we define as ``topics", generates the data (words and documents) we observe. The observational units are defined to be the words we see in the documents. The set of all documents, $\mathcal{D}$, is said to be the  which are members of the corpus. We denote the size of the corpus $D$, indexing documents by $d \in \{1...D\}$. Each document has a number of words, denoted by $w_{n,d}, for n \in \{1...N_d\}$, where $N_d$ is the number of words in the $d$-th document. Abstracted from the corpus is the vocabulary, a vector of all the unique words and features that are observed within the corpus. The terms in the vocabulary are denoted by $v \in \{1...V\}$, where features are derived from documents. There are many possible ways to derive features. In this setting, we will only include selected digrams as features. Digrams are word strings of length 2, such as ``vice president" or ``social security", which convey meaning in this context beyond the individual items such as  ``vice" and ``president" or ``social" and ``security" separately. Lastly, based on some trial and error, or perhaps based on other studies, we specify the number of topics, denoted $k \in \{1...K\}$.

Tops are latent units of meaning exist within the corpus, as non exclusive groups of words which have high co-occurrence within documents. Words, as members of the vocabulary, are not exclusive to any particular topic, but rather can occur in multiple topics with varying prevalence, something desirable for languages in which some words can have multiple unrelated meanings. Once the number of topics is decided and the set of topics is computed, we can then estimate the proportions of each topic in each document, as well as how our selected covariates affect these proportions.

\subsection{Data Generating Process}

The data generating process is assumed as follows:

\begin{itemize}
\item For $d \in \{1...D\}$, draw $\theta_d$, an element of the K-simplex (I.E. $\{x \in \R^k | 0\leq x_i\leq 1, \sum^K_{i=1}x_i=1\}$ ), specifying a particular vector of probabilities for the available topics.

\item For $w_{d,n}$, with $d in \{1...D\}$, and $n \in \{1...N_d\}$, draw a topic $z_{d,n} $ in \{1...K\}, a vector specifying probabilities over words in the vocabulary.

\item For $w_{d,n}$, with $d in \{1...D\}$, and $n \in \{1...N_d\}$, draw a specific word $v$ from the vocabulary. 
\end{itemize}

Based on this process, inference on the structure of the $K$ topics flows in reverse, starting from the observed words and working back identify the distributions over the vocabulary and the effects the covariates have on the rates of those topics appearing in documents and in the corpus. 

Most topic models use Bayesian methods, and thus require prior distributions be specified for analysis. However, where LDA, operating on the assumption that a single data generating process undergirds the entire corpus (in which documents are exchangeable with one another), and thus places a single global Dirichlet prior on the Multinomials that select the topics, STM instead assumes that arbitrary groups of documents specified by levels of a corvariate or set of covariates are governed by a set of priors with a Logistic Normal Distribution. This is what is presumed to make some documents in group (I.E. covariate level) A to be more about topic a whereas documents in group B can be more about topic b. 

%%stm moves the assumption of exchangability from the level of all documents in the corpus, to subsets of the documents specified by levels of the covariates.

%\subsection{Words, Topics, and Documents} %%maybe change subsection title to definitions?

%Within the framework of topic models generally, and the Structural Topic Model specifically, we consider each document as a response, which we define as the $d$-th instance within the corpus. We define the corpus as the entire collection of documents (responses) of interest. Then we define the vocabulary of that corpus to be the vector of all unique words or features present within the corpus after precprocessing (which is discussed in the following section). 

%Topics represent the unseen semantic units which contribute to generation of the observed documents. We are interested in capturing these topics via the words observed in each document.%%topics can be thought of as functions which  mediate word selection?

%To generate a document $d$, we first draw a topic distribution from the general set of all the topics available in the corpus (parameterized by the Logistic Normal, instead of the Dirichlet, which allows the prevalence covariates to influence the topic proportions), resulting in a multinomial distribution $\theta_d$ for that document, which specifies the proportions of each topic in that document. Then for each ``slot" (I.E. the words we will observe once the document is generated) in the document, we draw a topic $z_{d,n}$ from a multinomial parameterized by $\theta_d$ to determine what topic that slot will pertain to. Then, having determined which topic the $n$-th slot will be about, we draw a word from the vocabulary based on the probabilities $z_{d,n}$ (which can also be influenced by $\beta_{d,k,v}$) assigns to selected members of the vocabulary, resulting in the observed $w_{d,n}$.

%Get around exchangability issue by giving groups of documents within a covariate their own prior. 
%The inclusion of document level covariates means that documents are no longer exchangeable, since a document from level $a$ of covariate A cannot be exchanged with a document from level $a+1$ of the same covariate. However, this issue is addressed by moving from a global Dirichlet prior specified for all documents (as in LDA) to the use of a Logistic Normal prior with a mean which varies by covariate levels.

\subsection{General Model Specification}



The Structural Topic Model is essentially a combination and generalization of three separate topic models, the Core Language Model, which builds off the Correlated Topic Model \citep{Blei:2007jy}, the Topic Prevalence Model, developed in \citep{TMCAFDMR}, and the Topical Content Model, developed in \citep{SAGM}. Analysts can opt to leave out one of the latter two components if not pertinent to their questions of interest \citep{stm2016}.

Presuming the use of both prevalence and content covariates, for a corpus of size $D$ documents,  observed words $\{w_{d,n}\}$ in those documents, a specified number of topics $K$, vocabulary of size $V$, a design matrix of topic prevalence covariates denoted $\mathbf{X}$, and another design matrix of topical content covariates denoted $\mathbf{Y}$, the distributions involved are as follows:

\begin{centering}

\begin{align}
	\gamma_{k} &\sim \text{Normal}_P(0, \sigma^2_kI_P), & for \, k=1...K-1 \\
	\sigma^2_k &\sim \text{Inverse-Gamma}(a,b )\\ 
	\mathbf{\theta}_d &\sim \text{LogisticNormal}_{K-1}(\mathbf{\Gamma'}\mathbf{x}'_d,\mathbf{\Sigma}) \\
	\mathbf{z}_{d,n} &\sim \text{Multinomial}_K(\mathbf{\theta}_d) & for \, n=1...N_d\\
	\mathbf{w}_{d,n} &\sim \text{Multinomial}_V(\mathbf{B z}_{d,n}) &  for \, n = 1...N_d\\
	\beta_{d,k,v} & = \frac{exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c)}_{y_d,v} + \kappa^{(i)}_{y_d,k,v } )} {\Sigma_v exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c)}_{y_d,v } + \kappa^{(i)}_{y_d,k,v} ) } & for \, v=1...V \, and \, k=1...K
\end{align}
\end{centering}

$\gamma_k$ is a vector with length P, where P is the dimensions of prevalence covariate levels and interactions. The individual $\gamma_k$s together form $\Gamma$, a $P \times (K-1)$ matrix of coefficients that specify which cofactor levels influence the prevalence of which topics.

The full posterior involved in the model is $p(\mathbf {\eta, z,\kappa, \gamma, \Sigma} |\mathbf{w, X, Y }) \propto$

\begin{equation}
\bigg(\prod^D_{d=1}Normal(\mathbf{\eta_d}|\mathbf{X}_d\mathbf{\gamma}, \mathbf{\Sigma}) \Big(\prod^N_{n=1}Mult(z_{n,d}|\mathbf{\theta}_d) \times Mult(w_n|\mathbf{\beta}_{d,k=z_{d,n}} \Big) \bigg) \times \Pi p(\kappa) \Pi p(\mathbf{\Gamma})
\end{equation}


\subsection{Estimation}


%%part of the estimation process involves reducing the dimension of the corpus matrix, which has D rows and V columns (one column per term in the vocabulary), into a matrix which has D rows and K columns (one column per topic.

The posterior given in the previous subsection is intractable to any closed form evaluation. \citet{stm2016} utilize an approximate variational Expectations Maximization (EM) developed by \citet{WangBlei2013}. This involves finding the approximate posterior $\Pi_d q(\mathbf{\eta}_d)q(\mathbf{z}_d)$ where $q(\mathbf{\eta}_d)$ is a Gaussian with mean $\lambda_d$ and covariance $v_d$ and $q(\mathbf{z}_d)$ is a multinomial with parameter $\phi_d$, where $\phi_{d,n,k} \propto exp(\lambda_{d,k})\beta_{d,k,w_n}$ and where $\lambda_k$ is an outcome variable of the M-step.

In E-step, the algorithm, iterate through each document, updating variational posteriors $q(\mathbf{\eta}_d)$ and $q(\mathbf{z}_d)$. In the M-step, the algorithm maximizes the the approximate ``Evidence Lower Bound" with respect to $\mathbf{\Gamma}$, $\mathbf{\Sigma}$, and $\mathbf{\kappa}$ and updates coefficients in the topic prevalence model, topic model, and global covariance matrix.

As with other EM algorithms, this is iterated until convergence is said to be met when changes are smaller than some (small) specified value.

\section{Application}

\subsection{Data: American National Election Studies}

The American National Election Studies has been collecting survey data on elections in the United States since 1948. In addition to numerous traditional survey forms, they have also continuously asked several questions which elicit open ended responses from participants. These include the ``like/dislike" questions, which have been collected both on the two major US political parties and their congressional candidates during most presidential and midterm elections as well as for the parties' presidential candidates during presidential election years.  For this analysis, we focus on the ``like/dislike" responses specifically for presidential candidates of the two major parties, which read as follows:

\begin{quote}
Now I'd like to ask you about the good and bad points of the two candidates for President. Is there anything in particular about [Democratic presidential candidate]/[Republican presidential candidate] that might make you want to vote \textit{for} him?  What is that? Anything else?
\end{quote}

\begin{quote}
Is there anything in particular about [Democratic presidential candidate]/[Republican presidential candidate] that might make you want to vote \textit{against} him? What is that?  Anything else?
\end{quote}

While the ANES has collected open ended responses to these questions over many years, little has been done with them due to the high cost of analysis referenced above. We apply the Structural Topic Model to a subset of these open ended surveys, collected during presidential election years, from Ronald Reagan's reelection campaign of 1984 to George W. Bush's reelection campaign of 2004. Although the ANES also collected similar responses on the independent candidate, Ross Perot, in 1992 and 1996, we have omitted these data from the analysis to be consistent across all six years. In addition to the open ended survey responses, the ANES also collects basic demographic information on each respondent. As potential prevalence covariates, we explore the year the responses were collected, as well as the respondents' race, gender, political affiliation, church attendance, educational attainment, income percentile, and ideology.

\subsection{Text Pre-Processing}

To prepare the open ended survey responses for analysis, we first applied some standard text preprocessing. Although the STM package includes some functions from the text mining (tm) package, we opted to use the quanteda package, since it offered a bit more flexibility and its output could easily be converted into an object which the stm package could utilize. This process involved several steps:
\begin{enumerate}
\item remove punctuation
\item stemming
\item partial digramming
\item stop word removal (I.E., removal of generally very high frequency features)
\item very low frequency feature removal
\end{enumerate}


Stemming (2) ensures that words like ``immigrant" and ``immigrants" and ``immigration" are recognized as a single feature within the corpus, rather than three separate features. An n-gram is an $n$ length string of words of length $n$. As noted previously, ``partial digramming" (3) retains most features in the vocabulary as single words (I.E. unigrams), but blends certain phrases of two word length into single features, so that, for example, "vice president" or "social security" are recognized as single features, rather than two separate features, and thus are "seen" by our model as a different features than "president" or "security" respectively. We opted for "partial digramming" rather than using digrams of the entire corpus, since models using just unigrams or all uni and digrams generated fairly similar models, but topic estimation for models using all possible uni and digrams took significantly longer amounts of time to run. Stop word removal involves filtering out a standard set of English "function" words, including most articles and prepositions, as well as some corpus specific features, like "ae" which indicates that the interviewer asked "Anything else?" Lastly, very low frequency words ($n \leq 3$ instances) which might slow computation but add nothing to the interpret ability of the model are also removed. 

Two example responses are given below, showing first the unprocessed response followed by a ``toy" document feature matrix representing a corpus of only that individual document. Document feature matrices representing corpora with more than one document will almost always have numerous entries which are zero, since two documents will rarely if ever be represented by exactly the same vocabulary. 

Example 1: Original text
<<ex1orig, eval=TRUE, echo=FALSE>>=
print(str_break(example1))
@

Example 1: Preprocessed text
<<ex1prepro, eval=TRUE, echo=FALSE>>=
rbind(example2@Dimnames$features, example2@x)
@

Example 2: Original text
<<ex2orig, eval=TRUE, echo=FALSE>>=
print(str_break(example3))
@

Example 2: Preprocessed text
<<ex2prepro, eval=TRUE, echo=FALSE>>=
rbind(example4@Dimnames$features, example4@x)
@

After this preprocessing of the corpus, the remaining dataset contains 23,507 instances of open ended responses with counts for 3,357 features (1-grams and selected digrams). This corresponds to 9,977 individuals who responded to between one and four of these questions over the course of six presidential elections, taking place every four years between 1984 and 2004.

After this process, the corpus is then represented by a 23,507 rows by 3,357 columns document feature matrix, in which the $d$-th row corresponds to the $d$-th document and the $v$-th column represents the $v$-th feature in the vocabulary. Then the $d,v$-th entry in the matrix represents the number of times the $v$-th feature occurs in the $d$-th document. This matrix is very sparse, with the vast majority of entries being zeros.

\subsection{ANES Models}

After exploring simple models with various numbers of topics, we settled on models using $K=60$ topics. This is close to the number of topics ($K=69$) which \citet{stm2013} fit for their analysis of a open ended survey data from the ANES for the 2008 election, inquiring about the top issues of the campaign, for which they were also able to obtain and compare hand coded analysis.

As a baseline model, we fit the election year as a covariate. Rather than fitting this as a linear coefficient, we opted for a categorical variable to avoid forcing the prevalence of a given topic to either strictly increase or strictly decrease over time. This model is similar to the model presented in \citet{dtm2006}.

In addition to the year and respondent's party ID, further models fit one of the selected covariates (respondent's race, gender, income percentile, educational attainment, church attendance, or ideology). While our intention had been to examine how topics and topic proportions differ when models with different covariates are fit, none of the models fit appear to be particularly different. Models fit with different covariates yielded essentially the same topics, with minor differences in word prevalence within topics or topics within documents when compared via screeplots or word clouds. However, these visualizations, which still yield valuable and interesting insights into the open ended survey corpus, will be discussed in the following section.

\section{Contextualizing the findings: Discussion and Visualization}

Perhaps the most fun and also the most frustrating part of this project was considering ways to summarize the information gleaned from fitting the various models. While it is our hope that as scholars continue to use and explore these procedures, more easily assessable metrics of fit will be developed, we focus mostly on graphical examinations at this juncture. We produce plots of topic proportions to show some variation in topic prevalence across years and also by selected covariates. We incorporate expert political science opinion of American elections via examination of wordclouds and exemplar responses for particular topics in order to identify which topics are semantically coherent and what those topics pertain to. We also produce screeplots to determine how particular words contribute to the structure of particular topics, as well as how particular topics contribute to the structure of the corpus or subsets of the corpus.

While there may be some effects demographic covariates have on topic proportions from respondents or how those respondents discussed certain topics, we were unable to detect a strong enough signal to say so any certainty or qualitative rigor. However, the models and the overall process of studying their output have produced opportunities for qualitative analysis and comparisons. Incorporating various visualization techniques, some insights from the models are discussed below.

Most of the visualizations done throughout this paper leverage the amazing power and flexibility of ggplot \citep{ggplot2} and other packages within tidyverse \citep{tidyverse} to ``wrassle" data into tidy structures and then make plots more dynamic and informative, something which is much harder to do efficiently when relying on the base R plotting functionality, as is the case with the STM package \citep{stmR}.

\subsection{Simultaneous plotting of multiple covariates}

To better visualize how topic prevalence varies within the corpus based on particular covariates, we used tidyr and dplyr to reparameterize coefficients, then plot multiple covariates simultaneously, rather than just one at a time, as with the stm/base R plotting functionality. Below we present board overviews of the topic distributions affected by respondent covariates, with comments on a selection of them.

For example, in Figure \ref{simcovs}, we get a broad overview of the behavior of topics across the entire corpus with respect to Election year, Candidate Party, and voter Party. From this broad overview, we can then focus on particular topics which exhibit some interesting behavior based on knowledge of events that happened during particular election years. We discuss their wordclouds as well in a subsequent subsection.

\begin{figure}[H]
<<simcovs, echo=FALSE, fig.width= 7.7, fig.height=10>>=
feelC.FX %>% # filter(topic==i) %>%
  filter(partyID == "Rep" | 
           partyID == "Ind" | 
           partyID == "Dem") %>%
  ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes( shape=partyQ, color=partyID), alpha=0.7) +
  # ylim(0,.09) +
  guides(shape=guide_legend(title="Candidate"), color=guide_legend(title="Voter")) +
  facet_wrap(~topicf, ncol=7) + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
  theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
  ggtitle("Mean Topic Proportions for each Election Year \n based on Parties of Voters and Candidates")
@
\caption{\label{simcovs} Overview of Topic Proportions across the corpus of ANES open ended surveys}
\end{figure}


\begin{figure}[H]
<<racecovs, echo=FALSE, fig.width= 7.7, fig.height=10>>=
race.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=race)) +
  guides(shape=guide_legend(title="Voter \n Race"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year based on Voter Party and Race")
@
\caption{\label{racecovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's race}
\end{figure}

\begin{figure}[H]
<<gendercovs, echo=FALSE, fig.width= 7.7, fig.height=10>>=
gen.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=gender)) +
    guides(shape=guide_legend(title="Voter \n Gender"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year based on Voter Party and Gender")
@
\caption{\label{gendercovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's gender identification}
\end{figure}

\begin{figure}[H]
<<educovs, echo=FALSE, fig.width= 7.7, fig.height=10>>=
edu.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
  filter(edu!="DK/NA") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=edu)) +
    guides(shape=guide_legend(title="Voter \n Education"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year based on Voter Party and Education")
@
\caption{\label{educovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's educational attainment}
\end{figure}

\begin{figure}[H]
<<inccovs, echo=FALSE, warning=FALSE, fig.width= 7.7, fig.height=10>>=
inc.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
  filter(income!="DK/NA") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=income)) +
    guides(shape=guide_legend(title="Income \n Percentile"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year based on Voter Party and Income Percentile")
@
\caption{\label{inccovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's income percentile}
\end{figure}

\begin{figure}[H]
<<idecovs, echo=FALSE, warning=FALSE, fig.width= 7.7, fig.height=10>>=
ide.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
  filter(ideof!="none") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=ideof)) +
    guides(shape=guide_legend(title="Voter \n Ideology"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year based on Voter Party and Ideology")
@
\caption{\label{idecovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's Ideology}
\end{figure}

\begin{figure}[H]
<<chucovs, echo=FALSE, warning=FALSE, fig.width= 7.7, fig.height=10>>=
chu.int2.FX %>%  
  filter(partyID == "Rep" | 
         partyID == "Ind" | 
         partyID == "Dem") %>%
  filter(church=="Ev.Week"|
           church=="Al.Ev.Week"|
           church=="Monthly"|
           church=="FewYear"|
           church=="Never") %>%
   ggplot(aes(x=yearf, y=beta)) +
  geom_point(aes(color=partyID, shape=church)) +
    guides(shape=guide_legend(title="Church \n Attendence"), color=guide_legend(title="Voter \n Party")) +
  facet_wrap(~topicf, ncol=7) + 
 # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Presidential Election Year") +
   theme(axis.text.x=element_text(angle=90), plot.title = element_text(hjust=0.5)) +
    ggtitle("Mean Topic Proportions for each Election Year \n based on Voter Party and Church Attendence")
@
\caption{\label{chucovs} Overview of Topic Proportions across the corpus of ANES open ended surveys based on voter's Church Attendance}
\end{figure}

After taking a ``bird's eye" view of the corpus and topic behavior, we can examine particular topics, based on their identification. For example, as we can see in Figure \ref{top59prop} , there is a spike in the prevalence of Topic 59 in the 2000 presidential election. Both candidates brought up education frequently during that election \citep{edu2000}, and we can see that it was a topic respondents were apt to bring up as something they liked, particularly if they were democratic voters. Perhaps because of this, education was one of the earlier agenda items which the George W. Bush administration attempted to tackle, when he worked ``across the aisle" with Democrats to pass the now infamous No Child Left Behind.

\begin{figure}[H]
<<top59prop, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
partyQc.FX %>%  filter(topic==59) %>%
  filter(partyID == "Rep" | 
           partyID == "Ind" | 
           partyID == "Dem") %>%
  ggplot(aes(x=yearf, y=beta)) +
  # geom_point(aes(fill=feel, shape=partyQ), size=3) +
  geom_point(aes( shape=feel, color=partyID)) +
    ylim(0,.08) +
  #  facet_wrap(~feel) + 
  # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Year")+
    ggtitle("Proportions by Respondent Party and Sentiment for Topic 59")
@
\caption{\label{top59prop} Proportions for Topic 59, regarding education}
\end{figure}

%things to note? variation of the economy (topic 1), yet still always above average as a topic of elections

As the president serves as a stabilizing role within the country, presidential candidates' health, especially with respect to age, is frequently a topic in presidential elections. As we can see in Figure \ref{top51prop}, topic 59 shows up at higher than ``background rates" in 1984, when Reagan was running for election after being the oldest person ever to serve in the office. In an attempt to diffuse the issue, he often joked about his age, for example, saying "I want you to know that also I will not make age an issue of this campaign. I am not going to exploit for political purposes my opponent's youth and inexperience" \citep{reagyouth}. The topic was also of higher prevalence in 1996, when relatively youthful sitting president Bill Clinton was challenged by much older Senator Bob Dole.

\begin{figure}[H]
<<top51prop, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
partyQc.FX %>%  filter(topic==51) %>%
  filter(partyID == "Rep" | 
           partyID == "Ind" | 
           partyID == "Dem") %>%
  ggplot(aes(x=yearf, y=beta)) +
  # geom_point(aes(fill=feel, shape=partyQ), size=3) +
  geom_point(aes( shape=feel, color=partyID)) +
    ylim(0,.08) +
  #  facet_wrap(~feel) + 
  # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Year")+
    ggtitle("Proportions by Respondent Party and Sentiment for Topic 51")
@
\caption{\label{top51prop} Proportion of Topic 51, regarding age}
\end{figure}

Topics 18 (Figure \ref{top18prop}) and 23 (Figure \ref{top23prop}) both appear to relate to Bill Clinton and scandals, although topic 18 shows up across three consecutive elections (both of Clinton's, as well as Gore's) whereas topic 23 appears to be concentrated mostly in 1996 during Clinton's reelection, and may be particularly focused on the Watergate Scandal.

\begin{figure}[H]
<<top18prop, echo=FALSE,warning=FALSE,  fig.width= 6, fig.height=3.5>>=
partyQc.FX %>%  filter(topic==18) %>%
  filter(partyID == "Rep" | 
           partyID == "Ind" | 
           partyID == "Dem") %>%
  ggplot(aes(x=yearf, y=beta)) +
  # geom_point(aes(fill=feel, shape=partyQ), size=3) +
  geom_point(aes( shape=feel, color=partyID)) +
    ylim(0,.08) +
  #  facet_wrap(~feel) + 
  # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Year")+
    ggtitle("Proportions by Respondent Party and Sentiment for Topic 18")
@
\caption{\label{top18prop} Proportions for Topic 18, regarding Clinton/Scandal}
\end{figure}

\begin{figure}[H]
<<top23prop, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
partyQc.FX %>%  filter(topic==23) %>%
  filter(partyID == "Rep" | 
           partyID == "Ind" | 
           partyID == "Dem") %>%
  ggplot(aes(x=yearf, y=beta)) +
  # geom_point(aes(fill=feel, shape=partyQ), size=3) +
  geom_point(aes( shape=feel, color=partyID)) +
    ylim(0,.08) +
  #  facet_wrap(~feel) + 
  # theme(legend.position = "none") + 
  ylab("Topic Proportion") +
  xlab("Year")+
    ggtitle("Proportions by Respondent Party and Sentiment for Topic 23")
@
\caption{\label{top23prop} Proportions for Topic 23, regarding Clinton/Scandal}
\end{figure}

%topic 42, about ``change" and ``newness" when democrats retook the white house after republicans holding it for 12 years.

%50 - last four years - in 1984-1996, referendum on incumbent. vs in 2000, gore distanced himself from Clinton, and in 2004, bush managed to make the election about cherry

%54 - is fairly high across all years, but campaigns are always talking about ``the middle class", also 29,, and social security/welfare

%55 - death penalty was a big issue in 2000 because of bush's record in Texas

\subsection{Screeplots}

To visually examine how elements of one level of the model contribute to higher elements of the model, we can plot the elements' contribution to the group against the rank of their contributions, essentially forming a screeplot. Thus we can examine how much particular words contribute to the make up of topics, as well as how much particular topics contribute both to the make up of parts of the corpus subset by covariates as well as to the overall make up of the corpus. Screeplots examining topic contributions to the corpus or subsets of the corpus are provided in the following. Selected screeplots examining word contributions to topics are also examined below. The remaining screeplots examining word contributions to topics are provided in the appendix.

\begin{figure}[H]
<<sentbycandScree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=4>>=
scree2 %>% 
  nest(-mood) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(mood, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~mood) +
  labs(x = 'Order', y = 'Proportion contributed') + 
  ggtitle("Screeplots for topics separated out by Voter Sentiment and Candidate's Party")
@
\caption{\label{moodscree} Screeplot indicating topic contributions to corpus, separated out by levels of Voter Sentiment and Candidate's Party }
\end{figure}

From Figure \ref{moodscree}, we can observe not just what topics are being talked about the most in the corpus, but by who and what in regards to. So, for example, if respondents were asked what they disliked about the Democratic candidates, the most likely topic to come up was Topic 53, which references Dukakis and the Prisoner Release program from his home state of Massachusetts. Although first brought up during the 1998 Democratic primary by Al Gore \citep{horton1}, this issue was made particularly salient in the 1988 general election by the infamous ``Willie Horton" ad \citep{horton2}. By contrast, when asked what they most liked about the Democratic candidates, the most common topic is Topic 54, which references the middle and working class.

When asked about what they disliked about the Republican candidate, the most common topic was Topic 25, which references Taxes. It came up most in the 2004 election, after bush lowered taxes 2001 and again in 2003 \citep{bushtaxcuts}, benefiting mostly those Americans with income above the 95th percentile ($\geq \$150,000$) \citep{census2002}.  When asked what they liked about the Republican candidates, Topic 1, referencing the economy, was the most prevalent. 

\begin{figure}[H]
<<genderscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=2.5>>=
scree2 %>% 
  nest(-gender) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(gender, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~gender) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{genderscree} Screeplot indicating topic contributions to corpus, separated out by voter's gender }
\end{figure}

From Figure \ref{genderscree}, we can see that women appear to be more concerned than men with topic 54, about the Middle and Working Class, although men also discuss this topic at a fairly high rate. 

\begin{figure}[H]
<<racescree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=2.5>>=
scree2 %>% 
  nest(-race) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(race, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~race) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{racescree} Screeplot indicating topic contributions to corpus, separated out by voter's race }
\end{figure}

\begin{figure}[H]
<<eduscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=4>>=
scree2 %>% 
      filter(edu!="DK/NA") %>%
  nest(-edu) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(edu, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~edu) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{eduscree} Screeplot indicating topic contributions to corpus, separated out by voter's education }
\end{figure}

From Figure \ref{eduscree} we can see that for those with high school or less education, topic 54, concerning the Middle and Working Class, is further above other topics. Whereas for those with a bachelor's degree or beyond, there appears to be less priority on this topic, and there is not a large separation of any topics that ``run away with the conversation", so to speak.

\begin{figure}[H]
<<sentscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=2.5>>=
scree2 %>% 
  nest(-feel) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(feel, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~feel) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{sentscree} Screeplot indicating topic contributions to corpus, separated out by voter's sentiment }
\end{figure}

From Figure \ref{sentscree}, we are more likely to see respondents bring up The Working or Middle Class (Topic 54) or The Economy (Topic 1) if they like a candidate, whereas they are more likely to bring up Taxes (Topic 25) or Dukakis and the Massachusetts Prisoner Release program (Topic 53) if they dislike a candidate.

\begin{figure}[H]
<<idescree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
scree2 %>% 
    filter(ideof!="none") %>%
  nest(-ideof) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(ideof, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~ideof) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{idescree} Screeplot indicating topic contributions to corpus, separated out by voter's ideology }
\end{figure}

\begin{figure}[H]
<<incscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
scree2 %>% 
    filter(income!="DK/NA") %>%
  nest(-income) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(income, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~income) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{incscree} Screeplot indicating topic contributions to corpus, separated out by voter's income }
\end{figure}

Figure \ref{incscree} suggests that as voters' income percentile increases, concerns about the Middle and Working Class and the Economy decrease in importance and other Topics, such as Conservative Values (topic 14), Foreign/Generic Policy (topic 11), and Defense Spending (topic 3) rise to the top of their issues.

\begin{figure}[H]
<<chuscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3.5>>=
scree2 %>% 
     filter(church=="Ev.Week"|
           church=="Al.Ev.Week"|
           church=="Monthly"|
           church=="FewYear"|
           church=="Never") %>%
  nest(-church) %>% mutate(
    normed = purrr::map(data, normy)
  ) %>% select(church, normed) %>% unnest() %>%
  ggplot(aes(y=total, x=ord)) +
  geom_text_repel(aes(label=ifelse(ord<11, tops, NA))) +
  geom_point() +
  facet_wrap(~church) +
  labs(x = 'Order', y = 'Proportion contributed')
@
\caption{\label{chuscree} Screeplot indicating topic contributions to corpus, separated out by voter's churce attendence }
\end{figure}

From Figure \ref{top7scree}, we can see the top 40 words for Topic 07, which pertains to Vice Presidential Nominees and Running Mates, where height indicates how much the word shows up (unnormalized) and red hue indicates the weighted FREX calculation, which is described in the next subsection, where we discuss this in conjunction with Figure \ref{top7FREX}.

\begin{figure}[H]
<<top7scree, echo=FALSE, warning=FALSE,fig.width= 6, fig.height=3>>=
feel.FREX %>% filter(topic==7) %>% 
  arrange(desc(proportion)) %>% slice(1:40) %>%
  ggplot(aes(x=seq(1:length(proportion)), y= proportion, label=words)) + 
  geom_point(aes(color=frex)) +
  geom_text_repel(aes(color=frex, label=ifelse(seq(1:length(proportion))<31,
                                               as.character(words),'')), force=2.5) +
    scale_color_gradient(low = "black", high="red") + 
    theme(legend.position = "none")+ 
  labs(y="Contribution to Topic", x="Order")+
    ggtitle("Screeplot for Topic 07")
@
\caption{\label{top7scree} Screeplot for topic 07, about Vice Presidential Nominees and Running Mates}
\end{figure}

\subsection{Wordclouds}

Wordclouds are generated using ggplot \citep{ggplop}. For a given topic, we identify the highest frequency words, and display them in the plot, with word proportion mapped to size such that larger words imply a higher presence within that topic. However, as \citet{stm2013} and \citet{FREX2012} note, high frequency words alone may fail to capture the ``essence" of the topic, since high frequency words may also appear in other topics. To address this, \citet{FREX2012} developed a Frequency-Exclusivity metric, ``FREX", which can combine a particular word's frequency within a given topic and its exclusivity towards other topics. The FREX value for the $v$-th word and the $k$-th topic is computed by:

\begin{equation}
FREX_{k,v} = \Big(\frac{w}{ECDF(\beta_{k,v}/\sum^K_{j=1}\beta_{j,v})} + \frac{1-w}{ECDF(\beta_{k,v})} \Big)^{-1}
\end{equation}

where $w$ is a value chosen by the analyst to distribute weight between Exclusivity (left side addend) and Frequency (right addend) (we set it to $w=0.5$), and ECDF is the empirical cumulative distribution function, and $\beta_{k,v}$ indicates the probability of the $v$-th term given the $k$-th topic. 

In order to make our wordclouds more informative, we compute the FREX value for the entire vocabulary, conditioned on the topic, and then map those values to a two color gradient, so that the more exclusive a word is, the more red it will be in the graphic. Thus a term that is both frequent and exclusive will show up large and red, whereas a term that is frequent but less elusive will be a less vibrant shade of red, or even gray or black. Likewise, a term that is less frequent but still exclusive will be smaller, but can still be a brighter red. By combining both the overall prevalence of words within a topic and their FREX values, we present a more cohesive picture of each topic than either metric alone.

\begin{figure}[H]
<<top7FREX, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=3>>=
year.FREX %>% 
  filter(topic==7) %>% arrange(desc(proportion)) %>% slice(1:50) %>% 
      ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
      geom_text_repel(aes(color=frex),segment.alpha = 0, force = 1) +
      scale_size(range = c(3, 15), guide = FALSE) +
      scale_color_gradient(low = "black", high="red") +
      scale_y_continuous(breaks = NULL) +
      scale_x_continuous(breaks = NULL) +
      labs(x = '', y = '') +
      theme_classic() +
      ggtitle("Topic 07 colored by weighted Exclusivity")
@
\caption{\label{top7FREX} Wordcloud for topic 07, regarding Running Mates and Vice Presidential Candidates}
\end{figure}

From Figure \ref{simcovs} in a previous section, we can see that the rates for topic 07 are higher in 1984 and 1988. From Figure \ref{top7scree} and Figure \ref{top7FREX} we gather that this topic pertains to the selection of Running Mates and the Vice Presidency and its nominee. Notably, in 1984, running against incumbent Ronald Reagan, Democratic presidential nominee Walter Mondale nominated Geraldine Ferraro for his running mate, making her the first woman to be nominated to that role for a major party. This move on Mondale's part was heralded by some as a bold choice and criticized by others as an act of desperation. Presumably, the proportion of this topic remained high in 1988, since George Herbert Walker Bush, then the sitting Vice President, ran to succeed Reagan, his former running mate.  

\begin{figure}[H]
<<seltopsFREX, echo=FALSE, warning=FALSE, fig.width= 7.7, fig.height=5>>=
year.FREX %>% filter(topic==1 | topic==25 | topic==53 | topic==54) %>% nest(-topic) %>%
  mutate(vals = purrr::map(data, chop1)) %>% select(topic, vals) %>% unnest() %>% 
  mutate(topic = as.factor(topic),
         topic = fct_recode(topic,c(
           "economy" = "1",
           "Taxes" = "25",
           "Dukakis/Prisoner release" = "53",
           "Middle/working class" = "54"))) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=frex),segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient(low = "black", high="red") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  facet_wrap(~topic) +
  labs(x = '', y = '') +
  theme_classic() +
  ggtitle("Selected topics colored by weighted Exclusivity")
@
\caption{\label{seltopsFREX} Wordclouds for topics 1, 25, 53, and 54}
\end{figure}

The topics referenced in Figure \ref{moodscree} are shown in Figure \ref{seltopsFREX}.

Additional wordclouds are created to inspect the ways in which one of two content covariates influence word selection within a topic. We considered two models, one with Voter sentiment (like/dislike) (see Figures \ref{feelclouds1a}, \ref{feelclouds1b}, \ref{feelclouds2a},  and \ref{feelclouds2b}) assigned as the content covariate, and the other with Candidate Party (Democrat/Republican) (see Figures \ref{partyQclouds1a}, \ref{partyQclouds1}, \ref{partyQclouds2b}, and \ref{partyQclouds2b}) mapped to the content covariate, to see if either of these influenced \textit{how} respondents discussed topics. 

Here again, we map the proportion with which a word shows up in a given topic to the size of the term in the plot. But for a given term, now we map how much the term's frequency, conditioned on selected topic, content covariate level, and their interactions, deviates from the prevalence of that term across the corpus to a three color gradient, with gray being the central color. Terms that are not particularly ``biased" towards either level of the content covariate are mapped towards the center of the gradient, whereas terms that are more likely for one level or the other are mapped to the corresponding color. We show the most common terms in each topic in Figures \ref{feelclouds1}, \ref{feelclouds1b}, \ref{partyQclouds1a}, and \ref{partyQclouds1b}, but tip the scales towards more ``biased" terms in \ref{feelclouds2a}, \ref{feelclouds2b},  \ref{partyQclouds2a}, and \ref{partyQclouds2b}. 

\begin{figure}[H]
<<feelclouds1a, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
feel.clouds1 %>% filter(topic %in% 1:30) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="sentiment"),
                        breaks = c(-150000, 0,130000 ),
                        labels = c("like", "0", "dislike"),
    low = "purple", mid="gray", high="green") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf)+
  theme(legend.position = "none")
@
\caption{\label{feelclouds1a} Differences in vocabulary choice for sentiment Covariate for topics 1 through 30. Purple indicates like, whereas green indicates dislike. }
\end{figure}

\begin{figure}[H]
<<feelclouds1b, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
feel.clouds1 %>% filter(topic %in% 31:60) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="sentiment"),
                        breaks = c(-150000, 0,130000 ),
                        labels = c("like", "0", "dislike"),
    low = "purple", mid="gray", high="green") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf)+
  theme(legend.position = "none")
@
\caption{\label{feelclouds1b} Differences in vocabulary choice for sentiment Covariate for topics 31 through 60. Purple indicates like, whereas green indicates dislike. }
\end{figure}

\begin{figure}[H]
<<partyQclouds1a, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
partyQ.clouds1 %>%filter(topic %in% 1:30) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="candidate"),
                        breaks = c(-275000, 0,270000 ),
                        labels = c("Republican", "0", "Democrat"),
                        low = "red", mid="gray", high="blue") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")

@
\caption{\label{partyQclouds1} Differences in vocabulary for candidate for topics 1 through 30. Red indicates Republican candidate whereas blue indicates Democratic candidate.}
\end{figure}

\begin{figure}[H]
<<partyQclouds1b, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
partyQ.clouds1 %>%filter(topic %in% 31:60) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="candidate"),
                        breaks = c(-275000, 0,270000 ),
                        labels = c("Republican", "0", "Democrat"),
                        low = "red", mid="gray", high="blue") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")

@
\caption{\label{partyQclouds1b} Differences in vocabulary for candidate for topics 31 through 60. Red indicates Republican candidate whereas blue indicates Democratic candidate.}
\end{figure}


\begin{figure}[H]
<<feelclouds2a, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
feel.clouds2 %>%filter(topic %in% 1:30) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="sentiment"),
                        breaks = c(-150000, 0,130000 ),
                        labels = c("like", "0", "dislike"),
    low = "purple", mid="gray", high="green") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")
@
\caption{\label{feelclouds2a} Differences in vocabulary choice for sentiment Covariate for topics 1 through 30. Purple indicates like, whereas green indicates dislike. }
\end{figure}

\begin{figure}[H]
<<feelclouds2b, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
feel.clouds2 %>%filter(topic %in% 31:60) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="sentiment"),
                        breaks = c(-150000, 0,130000 ),
                        labels = c("like", "0", "dislike"),
    low = "purple", mid="gray", high="green") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")
@
\caption{\label{feelclouds2b} Differences in vocabulary choice for sentiment Covariate for topics 31 through 60. Purple indicates like, whereas green indicates dislike. }
\end{figure}

\begin{figure}[H]
<<partyQclouds2a, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
partyQ.clouds2 %>%filter(topic %in% 1:30) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="candidate"),
                        breaks = c(-275000, 0,270000 ),
                        labels = c("Republican", "0", "Democrat"),
                        low = "red", mid="gray", high="blue") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")

@
\caption{\label{partyQclouds2a} Differences in vocabulary for candidate for topics 1 through 30. Red indicates Republican candidate whereas blue indicates Democratic candidate.}
\end{figure}

\begin{figure}[H]
<<partyQclouds2b, echo=FALSE, warning=FALSE, fig.width= 8.2, fig.height=9.5>>=
partyQ.clouds2 %>%filter(topic %in% 31:60) %>%
  ggplot(aes( x=1, y=1, size=proportion, label=words) ) +
  geom_text_repel(aes(color=difs), segment.alpha = 0, force = 1) +
  scale_size(range = c(3, 15), guide = FALSE) +
  scale_color_gradient2(guide=guide_colorbar(title="candidate"),
                        breaks = c(-275000, 0,270000 ),
                        labels = c("Republican", "0", "Democrat"),
                        low = "red", mid="gray", high="blue") +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  labs(x = '', y = '') +
  theme_classic() +  facet_wrap(~topicf) +
  theme(legend.position = "none")

@
\caption{\label{partyQclouds2b} Differences in vocabulary for candidate for topics 31 through 60. Red indicates Republican candidate whereas blue indicates Democratic candidate.}
\end{figure}

At the suggestion of \citet{stm2016}, while in theory we could assign a more complicated set of  covariates to the content covariate of the model, going from no content covariate to a categorical content covariate with two levels doubles the number of parameters of the model. Two content covariates each with two factor levels would quadruple the number of parameters, and so forth. Thus, as we explored the STM, in the models we tried which incorporated content covariates, we opted for covariates that only had at most two factor levels which exhaustively captured all documents. 

\section{Conclusions and Future areas of Interest}

Topic Modeling is a fascinating cutting edge area of develoipment in machine learning and natural language processing. The Strucutral Topic Model in particular offers an exciting and versatile tool for researchers in the Social Sciences and Humanities. However, the relative novelty of STM coupled with its flexiblity is simultaneous blessing and a curse. Even with a deeper knowledge of stats, examining/understanding/interpreting models is not entirely straight forward. As it currently stands, many social scientists and humanities scholars may lack the necessary background knowledge to truly utilize STM's potential, particularly if their models of interest have any complexity.

To that end, more research is need to develop methods to assess and quantify model and parameter fits. Some way to compare similar models, such as models which are nested within one another or models with only slightly different numbers of topics, could greatly aid in this endevour. For the time being, it is unclear if adding covarites to the models better or worse, nor is it clear what ``better" or ``worse" even mean in this context. We remain hopeful that as more scholars continue to explore topic models in general and STM in particular, that more straightforward diagnostics will be developed.

As with statistics broadly, some of the craft is an art more than a science. The selection of topic number currently lives more firmly in the former, but hopefully it will move towards the latter with time. Perhaps with the continuing exponential growth of computing power, topic model fitting will continue to grow, and the concerns outlined above will become trivial. This may become particularly true if methods could be developed to parallelize some of the computations that are currently required to estimate STM and other topic models. 

One idea to incorporate additional diagnostics into topic modeling, to assess if the models are truly recovering coherent topics, could be through the use of Visual inference, as suggested in work like \citet{vizinf}. For example, in addition to showing topics to political science experts, topic assessment could be crowd sourced through services like Amazon's Mechanical Turk, where it is fairly easy to gather large numbers of responses from lay people in a fairly short amount of time, and for relatively low costs. Such an endevour might involve selecting a topic from the model, then generating multiple random ``dummy" topics, generating word clouds for each, and then seeing if the respondents are able to recover the correct topic with any level of reliability. Desired $\alpha$ levels can be attained by generating $n$ dummy topics such that $\frac{1}{n+1} = \alpha$, creating an analog to a hypothesis test. Such an experimental design need not even be limited to lay people, and could be given to scholars in political science as well. 

But despite its limitations and complications, this avenue of exploration has great promise .
\newpage 
\bibliographystyle{asa}

\bibliography{references}

\newpage
\appendix

\section{Screeplots for All Topics}

<<allscree, echo=FALSE, warning=FALSE, fig.width= 6, fig.height=2.6>>=
for (i in 1:60){
  print(feel.FREX %>% filter(topic==i) %>% 
    arrange(desc(proportion)) %>% slice(1:40) %>%
    ggplot(aes(x=seq(1:length(proportion)), y= proportion, label=words)) + 
    geom_point(aes(color=frex)) +
    geom_text_repel(aes(color=frex, label=ifelse(seq(1:length(proportion))<31,
                                               as.character(words),'')), force=2.5) +
    scale_color_gradient(low = "black", high="red") + 
      labs(x="Order", y="Contribution") +
    theme(legend.position = "none")+ 
    ggtitle(paste("screeplot for topic", i, sep = " "))
  )
}
@



\end{document}