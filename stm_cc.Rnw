\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\title{A tbd title for STM paper}
\author{Joseph Eduardo Papio}

\begin{document}


\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE, cache=TRUE, width=40)

library(tm)
library(ngram)
#library(RWeka)
library(quanteda)
library(RColorBrewer)
library(stm)
library(stringi)
library(Rtsne)
library(geometry)

set.seed(11123)

dfTranscripts <- read.table("transcriptsB.csv",header=F,sep="\t",colClasses=c("character","character"), col.names=c("url","text"),quote="")

str_break = function(x, width = 70L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}
@

\section{Introduction and Literature Review}
 
Open ended survey responses provide an opportunity to solicit unguided opinions and ideas from participants, avoiding possible biases that can be introduced when utilizing more traditional survey forms. More traditional survey forms, on the other hand, require participants to select from several predetermined options in a closed set of choices. 

The immediate benefit of the traditional survey forms is that once the data are collected, analysis, including estimation of the prevalence of those opinions within the population, is fairly straight forward, and generally requires only relatiely low amounts of personnel and computing time. 

Until the last decade or so, little existed in the way of automated processing processing or analysis of open ended surveys. Identification and analysis of the content of the surveys often required a large number of person-hours, in which trianed readers went through individual responses, identifying the presence or absence of one or more topical areas from a predetermined list. Thus, in addition to the large resource requirements, some topical information contained in the responses might still be lost when utilizing this method of analysis if that topical information didn’t fit neatly into one of those predetermined areas. %Which is to say, the application of of this processing via human readers might introduce bias into the results through the predetermined list of topic options.

%Methods of automated textual analysis 
%Topic models-Application of Latent Dirichlet Allocation to the process of automatic/computer aided textual analysis known as Topic Models. 

%To facilitate analysis, surveys are often collected in such a way as to give respondents a choice from one of several preselected options, since analysis of open ended survey responses can be very time and labor intensive, requiring human coders to read through the indvidual responses, make judgements, and then code what thematic information is contained within them. 

%Enter topic models, pLSI (Hoffman) then LDA (Blei)
Topic models attempt to bring some reliable level of automation to quantifying the thematic information present in the text responses collected through open ended surveys. We treat each response as a document, which we define as the i-th instance (exemplar?) within the corpus. We define the corpus as the entire collection of documents (responses) of interest. 

Topics, then, exist within the corpus, as groups of words which have high co-occurrence within documents. Once topics are computed, then the proportion of each topic within each document can be estimated.

(discussion of pLSI here) \citep{hofmann:1999} % pLSI requires increasing the number of parameters as the number of documents within a corpus increases, cannot assign probabilities beyond the dataset provided to fit the model, so can’t apply things like test/training set hold outs to test model, etc?

Latent Dirichlet Allocation (LDA) \cite{blei2007} builds on the work of pLSI, allowing for the assignment of probability beyond the documents included when the model was fit to documents not originally included. It also avoids the increase in the number of estimated parameters as the number of documents increases. Blei and  ___’s work 


%Topic models attempt to bring some reliable level of automation to quantifying the thematic information present in the text responses collected through open ended surveys. We define each response as a document and the specific set of survey responses as our corpus. To determine the thematic content of the documents and the corpus,  with the goal of estimating the proportions with which those themes, which we define as topics, occur within the documents of the corpus. %(loosely topics are themes, and more technically, they are non exclusive clusters of words. distance between/within clusters is (?) computed based on the frequencies of word (co?)occurance within/across documents ) 

 "Simple" Latent Dirichlet Allocation is the base form of this method, built on the common assumptions of exchangeability and independence. %exchangability - conditional independence, ith and jth instances can be switched, order doesn't matter
 Obviously, the order of the words as they appear does matter for English syntax, but LDA treats each document as a ``bag of words", essentially ignoring syntactic information and focusing entirely on the semantic information.
 Unsupervised classification problem in machine learning (topics are not known before hand)
 %\hh{define exchangeability and independence in the context of LDA}

%\hh{what data are LDA models dealing with? - give an example. the xkcd data would be good for the intro. }
%\hh{what is the goal of an LDA analysis? }

Unlike previous probabilistic methods to model documents, such as probabilistic latent semantic indexing \citep{hofmann:1999}, LDA is a (vast/stark?) improvement because it can assign probabilities beyond the documents included when the model was fit to documents not originally included. pLSI also requires increasing the number of parameters as the number of documents within a corpus increases.
%\hh{ give an example (with citation) of a previous method}

A major limitation of the original LDA is its inability to incorporate correlation across topics. 
In practice, independence between documents is an unrealistic expectation when dealing with large sets of documents. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that were published previously. 
Correlated topic models build on LDA by relaxing the independence assumption \citep{Blei:2007jy}. %(cite blei/lafferty 2007) %\hh{give the math of how this independence assumption is relaxed.} 

Structural topic models further build on the additions of correlated topic models by incorporating document level covariates, such as particular characteristics of the author(s) \citep{stm2013}. %\hh{please give an example here - it's getting very abstract.}

For example, in the ANES dataset, document level covariates include the respondent's gender, race, and/or party affiliation.

\citet{stm2013} apply their method of Structural Topic Modeling %\hh{what is their method?} 
to open ended survey data from the ANES, collected in 2008. We futher apply their method to a larger amount of data from the ANES, open ended survey questions collected every presidential election year from 1948 through 2004. In addition to the open ended survey responses, basic demographic information was also collected, including the respondents' race, gender and political affiliation.

% with LDA, priors for topics and words are dirichlets, whereas with STM, priors for toopics are Normal-Gamma topic and Gamma Lasso for kappa 


\section{Methods}
apply LDA (similar to xkcd) to ANES data without taking respondent information into account
then apply STM to ANES data, taking respondent information into account and compare

\section{Applications}

\subsection{xkcd}

The xkcd corpus consists of transcriptions of the first 1265 comics and artworks produced at xkcd.com. To act as a meta-document covariate, a dummy variable ``group" was added, dividing the corpus into 11 groups of 115 documents each by simply taking the first 115 documents and calling them ``group 1", the second 115 documents ``group 2", and so forth. \hh{this group variable will capture some time component, right?}

Prior to fitting models to the open ended text corpus, the documents must first undergo some basic text preprocessing. For the xkcd dataset, documents begin as one or more sentences plus some descriptive text, a transcription:

<<examples1, echo=FALSE, tidy=TRUE>>=
xTran <- Corpus(VectorSource(dfTranscripts$text))

#inspect a few documents in the corpus
cat("Comic 30:")
print(str_break(as.character(xTran[[30]])))
cat("Comic 612:")
print(str_break(as.character(xTran[[612]])))

#store to demonstrate chnages as text processing progresses
ex1a <-as.character(xTran[[30]])
ex2a <-as.character(xTran[[612]])

#convert to lower case
xTran <- tm_map(xTran, content_transformer(tolower))

ex1b <-as.character(xTran[[30]])
ex2b <-as.character(xTran[[612]])

#remove stopwords, including "alt-title", which is in most of the comics as the mouse over text
xTran <- tm_map(xTran, removeWords, c(stopwords("english"), "alt-title", "alt", "title"))

ex1c <-as.character(xTran[[30]])
ex2c <-as.character(xTran[[612]])

#remove punctuation and numbers
xTran <- tm_map(xTran, removePunctuation)
xTran <- tm_map(xTran, removeNumbers)

ex1d <-as.character(xTran[[30]])
ex2d <-as.character(xTran[[612]])

#stemming
xTran <- tm_map(xTran, stemDocument)

ex1e <-as.character(xTran[[30]])
ex2e <-as.character(xTran[[612]])

#remove any unnecessary white space
xTran <- tm_map(xTran, stripWhitespace)

ex1f <-as.character(xTran[[30]])
ex2f <-as.character(xTran[[612]])


@

Then, the text is converted to lower case, the numbers and punctuation are removed, English stop words are removed, and the words are stemmed so that different forms of base words, like singular and plural instances, or different tenses, are counted as the same word. Lastly, any additional spacing between words is removed, resulting in a string of base forms for each document:

<<print2, echo=FALSE, warning=FALSE, message=FALSE,  verbose=FALSE>>=
cat("Comic 30:")
print(str_break(ex1f))
cat("Comic 612:")
print(str_break(ex2f))
@

<<examples2, echo=FALSE, message=FALSE, verbose=FALSE, results='hide'>>=
#convert vector from dataframe into an corpus for quanteda package
myCorpus <- corpus(dfTranscripts$text)
docvars(myCorpus, "group") <- as.factor(rep(1:11,each=115))

#create document feature matrix, with 1-grams and 2-grams
mydfm <- dfm(myCorpus, stem=TRUE, ngrams=c(1,2), 
             removeNumbers=TRUE, removePunct=TRUE,
             removeSeparators=TRUE, ignoredFeatures=c(stopwords("english"), "title", "text"))

#remove words with less than 4 occurances and in only one document
mydfm <- trim(mydfm, minCount=4, minDoc=2)

#further trim to include 5 or more occurances and in at least two docs
mydfm <- trim(mydfm, minCount=5, minDoc=3)

stmdfm <- convert(mydfm, to="stm",
                  docvars=data.frame(group=docvars(myCorpus, "group")))

stmFit <- stm(stmdfm$documents, stmdfm$vocab, K=10, prevalence= ~group, data=stmdfm$meta, init.type="Spectral" )

@

After this, the corpus is converted into a document feature matrix consisting of the 1-gram and 2-gram tokens for each document. The \textit{d}-th row of this matrix represents the \textit{d}-th document, and the \textit{w}-th column represents the \textit{w}-th n-gram in the vocabulary. Then the \textit{d,w}-th entry in the matrix is the number of times the \textit{w}-th n-gram occurs in the \textit{d}-th document. During this process, highly common n-grams which occur in all or almost all the documents are trimmed, as are highly uncommon n-grams which occur in only a very few number of documents. This cuts down on computation time with minimal loss of useful information, since very high frequency words can tell us little about what distinguishes documents in order to estimate topics and their prevalence, while very low frequency words can tell us little about what topics documents have in common.

The resultant document feature matrix for the xkcd data set consists of 1265 rows, one for each document, and 2850 columns, one for each non trimmed n-gram remaining in the vocabulary. 

<<examples3, echo=FALSE>>=
as.matrix(mydfm)[c(30,612),40:50]

#Plot wordcloud
#plot(mydfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))


@

From here, the STM package (or LDA package) can be applied to the data, and topics and their proportions in the documents and the corpus can be estimated and displayed. 

\begin{figure}
<<examples4, echo=FALSE>>=
plot.STM(stmFit, type = "summary", xlim = c(0, .3))
@
\caption{\label{stm-plot} \hh{Plot result from plot.STM - this needs to have a better caption}}
\end{figure}

\subsection{ANES}

\section{Conclusion}

could also reference dynamic topic models (blei, lafferty 2006) which adds a temporal  element to the model, since data are collected over time. \hh{are you planning to model the temporal aspect? If not, this would be better in the conclusions and future work statement.}


\hh{you'll need to define every symbol and index that you use.}

\centering Topic Prevalence:
\begin{align*}
	\mu_{d,k} &= X_d\gamma_k \\
	\gamma_k &\sim \mathcal{N}(0,\sigma_k^2)\\
	\sigma_k^2 &\sim Gamma(s^{\gamma}, r^{\gamma})
\end{align*}

\centering Language Model:
\begin{align*}
\theta_{d} &= LogisticNormal(\mu_d, \Sigma) \\
z_{d,n} &\sim Mult(\theta_d)\\
w_{d,n} &\sim Mult(\beta^{k=z_d,n}_d, r^{\gamma})
\end{align*}

\centering Topical Content:
\begin{align*}
\beta_{d,v}^k &\propto exp(m_v+\kappa_v^{.,k}+\kappa_v^{y,.}+\kappa_v^{y,k}) \\
\kappa_v^{y,k} &\sim Laplace(0,\tau_v^{y,k})\\
\tau_v^{y,k} &\sim Gamma(s^{\kappa}, r^{\kappa})
\end{align*}






\bibliographystyle{asa}
\bibliography{references}


\end{document}
