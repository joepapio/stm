\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}

\setlength{\parskip}{.5em}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\newcommand{\jp}[1]{{\color{blue}#1}}


\title{What Are They Talking About?\\
\vspace*{1\baselineskip}
\large Applying the Structural Topic Model to Open Ended Survey Responses from the American National Election Studies}

\author{Joseph Eduardo Papio\\
\vspace*{1\baselineskip}
\small Iowa State University}

\begin{document}

\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE, cache=TRUE, width=40)
library(tidyverse)
library(quanteda)
library(RColorBrewer)
library(stm)



str_break = function(x, width = 70L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}

example1 <-"He has turned his back on the coal and steel industry in favor of importing from foreign countries.  He has West Virginia, Ohio, and Pennsylvania on its knees."

example2 <- dfm(example1, stem=TRUE, removeNumbers=TRUE, 
                 removePunct=TRUE, removeSeparators=TRUE,
                 ignoredFeatures=c(stopwords("english"), 
                                   "na", "ao", "wm", "tm", "ae", "ge"))

example3 <- "I think my husband's disability and our social security is in danger.  He will cut this and the disability is my husband's only income.  He has already cut some out of our check."

example4 <- dfm(gsub("social security", "socialsecurity", example3), stem=TRUE, 
                removeNumbers=TRUE, removePunct=TRUE, removeSeparators=TRUE, 
                ignoredFeatures=c(stopwords("english"), 
                                   "na", "ao", "wm", "tm", "ae", "ge"))
@

\section{Introduction, Motivation, and Literature Review}

\subsection{Open Ended versus Traditional Surveys}
 
Open ended survey questions provide an opportunity to solicit unguided opinions and ideas from participants, avoiding possible biases that can be introduced when utilizing more traditional survey forms in which respondents are prompted to select their answer from one of several predetermined options in a closed set of choices. 

The immediate benefit of such traditional survey formats is that once the data are collected, the analysis, including estimation of the prevalence of those opinions within the population, is fairly straight forward, and generally requires only relatively low amounts of personnel power and computational resources. However, by prompting respondents to select from among these predetermined choices, which must be decided upon by researchers prior to implementation, ideas which are already present in the political collective consciousness but not yet salient to researchers may be papered over and thus lost. Considering such information loss, exploration and application of of open ended (and thus "unprimed") survey responses is an area of great interest and potential in the humanities and social sciences.

Unlike analysis of more traditionally formatted surveys, open ended surveys can be much more resource intensive to analyze. Prior to analysis, the data often require large amounts of in person processing to read and code the responses. Readers must be trained before they can perform the task of reading through indiviudal responses and then use their judgement to decide what topic or topics each open ended response is about, often selecting those topics from options generated by the researchers. Thus, in addition to the large resource requirements, some topical information contained in the responses may still go still go unrecognized when utilizing this method of analyzing open ended surveys if that topical information didnâ€™t fit neatly into any of the options predetermined by researchers or readers.

However, as computing power increases, interest in the development, exploration, and application of automated textual analysis methods such as topic models  continues to grow, especially for humanities and social science researchers. Such interest is likely to be stronger with researchers who are required to first build survey instruments before interacting with respondents, as the time required to construct the instruments prior to deployment can introduce significant lag. During such time, nascent ideas forming within the population may be missed or detected by other researchers.

%%in cutting edge areas of research, like election analysis, such applications could provided scholars with insights almost in "real time", potentially just days after the data are collected.

\subsection{pLSI, LDA, and STM}

Topic models are a subfield of Machine Learning and Natural Language Processing. Collectively they represent an effort to bring some level of automation, speed, and reliability to quantifying the latent thematic information present in text responses. Such efforts can be applied to texts of varying sizes, including the open ended sureys. Thus, in the past two decades of their development, they have great promise for reducing the resource burdens mentioned previously.

Probabalistic Latent Semantic Indexing (pLSI) was one of the earliest forms of topic modeling to be proposed  \citep{hofmann:1999}. pLSI is derived from latent class modeling for factor analysis of count data. It relies on associating latent class variables, ie topics, with how observed data (words) co-occur. Despite many limitations, pLSI was a pioneering step in the development of the foundation for the Topic Models framework. 

Latent Dirichlet Allocation (LDA) \cite{LDA2003} built on the work of pLSI. Blei, Ng and Jordan assume both documents within the corpus and words within the document are exchangable, which is to say the order does not matter, and any word/document can come before or after any other word/document. This treatment of words within a document is referred to as the "bag of words" approach.  Although this approach with respect to the words within a document causes some loss of syntactic information, particularly in a language like English where the word ordering often conveys meaning about the grammatical relationships between words, this is not a major concern for topic modeling, since the primary focus is on ascertaining the semantic and thematic content of the set of documents in question. 

%%  A Dirichlet distribution with alpha params $\leq 1$ pushes the generated multinomials towards an "edge" of the simplex such that unlike the "center" of the Dirichlet, probability is redistributed away from some topics and moved to other topics.  This is different from the assumptions made (or rather, not made) by pLSI, where all topics have some non negligible probablility for each document.

Blei further builds on LDA, addressing the somewhat unrealistic expectation with respect to large collections of documents that there is no correlation among topics or documents, with the introduction of Correlated Topic models \citep{Blei:2007jy}. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that were published previously. Thus, correlated topic models build on LDA by relaxing assumption that documents are independent and identically distributed.

Further developments in topic modeling include the Dirichlet-multinomial Regression model to allow covariates to affect topic prevalance \citep{TMCAFDMR} and change the rate at which a topic appears in a given document and allowing topic content to vary in log-frequency from a the background/mean \citep{SAGM}, thus allowing respondents to discuss a given topic at the same rate but in different ways (ie using different words).

Taking these developments and combining them, Structural Topic Models (STM) \cite{stm2013} offer a general framework to allow the inclusion of document level covariates which can influence either the rate at which a topic appears in a document \textit{or} what words are more or less likely to be used when a given topic is discussed by different groups. In the open ended surveys analyzed here, the covariates explored include the year of the election, demographic information about the respondent (race, gender, income, education, ideology, church attendence and party affiliation), the candidate's political party, and whether the thing the respondent mentions in their response is likely to make them vote \textit{for} (like) or \textit{against} (dislike) that candidate.

\citet{stm2013} apply their method of Structural Topic Modeling to open ended survey data from the American National Election Studies (ANES) collected in 2008 in which respondents were asked to identify the two most important political problems facing the United States and personal issues in the election. 


\section{Methodology}


\subsection{Words, Topics, and Documents}

Within the framework of topic models generally, and the Strucural Topic Model specifically, we consider each response as a document, which we define as the $d$-th instance within the corpus. We define the corpus as the entire collection of documents (responses) of interest. Then we define the vocabulary of that corpus to be the vector of all unique words or features present within the corpus. 

Topics represent the unseen semantic units which are contribute to generation of the documents we observe, and we are interested in capturing them by making inference on the observed data. They exist within the corpus, as non exclusive groups of words which have high co-occurrence within documents. Words, as members of the vocabulary, are not exclusive to any particular topic, but rather can occur in multiple topics with varying prevalence, something desirable for languages in which some words can have multipleunrelated meanings. Once the set of topics is computed, then the proportion of each topic within each document can be estimated, as well as how arbitrary covariates affect those proportions.

The structural topic model, like LDA, assumes an underlying process generates the data we observe, that is, the words in the documents. For a corpus of $D$ documents, select a document, $d \in \{1...D\}$. Each document has a number of terms, denoted by $n \in \{1...N_d\}$, where $N_d$ is the number of feature positions in the $d$-th document. Abstracted from the corpus is the vocabulary, a vector of all the unique features that are observed within the corpus. The terms in the vocabulary are denoted by $v \in \{1...V\}$. Lastly, based on some trial and error, or perhaps based on other studies, we specify the number of topics, denoted $k \in \{1...K\}$.

To generate a document $d$, we first draw a topic distribution from the general set of all the topics available in the corpus, resulting in a multinomial distribution $\theta_d$ for that document. Then for each "slot" (ie the words we will observe once the document is generated) in the document, we draw a topic $z_{d,n}$ from a multinomial parameterized by $\theta_d$ to determine what topic that slot will be about. Then, having determined which topic the $n$-th slot will be about, we draw a word from the vocabulary based on the probabilities $z_{d,n}$ assigns to selected members of the vocabulary.

%% for d in 1...D, draw \theta_d, a vector of probabilities that specifies some subset of the K latent topics. the \theta_d$s are influnced by the prevalance covariates specified from the $\mathcal{X}$ matrix.

%% then for each d in D, based on its \theta_d, for n in 1...N_d, draw a part particular topic, then draw a word from that topic's distribution over the vocabularly. This topic-word distribution, denoted $\beta_{d,k,v}$ is influnced by the content covariates specified in the $\mathcal{Y}$ matrix. This yields the w_{d,n}

%where LDA assumes an a global prior distribution over topics for all the documents in the corpus, STM assumes that arbitrary groups of documents specified by levels of a corvariate or set of covariates can have have different prior distributions over topics, thus allowing some documents in group A to be more about topic a whereas documents in groub B can be more about topic b.

%%ie LDA assumes the same data generating structure for the entire corpus, whereas STM allows the data generating process to vary


Get around exchangability issue by giving groups of documents within a covariate their own prior. 
The inclusion of document level covariates means that documents are no longer exchangable, since a document from level $a$ of covariate A cannot be exchanged with a document from level $a+1$ of the same covariate. However, this issue is addressed by moving from a global Dirichlet prior specifed for all documents (as in LDA) to the use of a Logistic Normal prior with a mean which varies by covariate levels.



\subsection{General Model Specification}

\jp{i've started fleshing this part out. some of it is still fragmented, and probably redundant. i'll keep working on it. i need some guidance on how in depth i need to talk about this in the paper and in the presentation}

The Structural Topic Model is essentially broken into three parts, the Core Language Model, which builds off the Correlated Topic Model \citep{Blei:2007jy}, the Topic Prevalence Model, developed in \citep{TMCAFDMR}, and the Topical Content Model, developed in \citep{SAGM}.

\citet{stm2016}

For a document $d$, given a selected number of topics $K$, observed words $\{w_{d,n}\}$ a design matrix of topic pravalence covariates denoted $\mathbf{X}$ and another design matrix of topical content covariates denoted $\mathbf{Y}$

\begin{centering}

\begin{align}
	\gamma_{k} &\sim Normal_P(0, \sigma^2_kI_P), & for \, k=1...K-1 \\
	\sigma^2_k &\sim Inverse-Gamma(a,b )\\
	\mathbf{\theta}_d &\sim LogisticNormal_{K-1}(\mathbf{\mu_d = \Gamma'}\mathbf{x}'_d,\mathbf{\Sigma}) \\
	\mathbf{z}_{d,n} &\sim Multinomial_K(\mathbf{\theta}_d) & for \, n=1...N_d\\
	\mathbf{w}_{d,n} &\sim Multinomial_V(\mathbf{B z}_{d,n}) &  for \, n = 1...N_d\\
	\beta_{d,k,v} & = \frac{exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c)}_{y_d,v} + \kappa^{(i)}_{y_d,k,v } )} {\Sigma_v exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c)}_{y_d,v } + \kappa^{(i)}_{y_d,k,v} ) } & for \, v=1...V \, and \, k=1...K
\end{align}
\end{centering}

$\gamma_k$ is a vector with length P, where P is the length of prevalence covariate levels and interactions (P increases for more complicated models with larger numbers of covariates, levels of those covariates, and interactions between covariates)

the individual $\gamma_k$s together form $\Gamma$, a $P \times (K-1)$ matrix of coefficients that specify which coefficients influence which topics

for each document, $\theta_d$ is drawn from a logistic normal, with mean and variance derived from the $\Gamma$ matrix

for each word in the document, $z_{d,n}$ is drawn from a multinomial parameterized by $\theta_d$, thus giving a single latent topic from wich the corresponding $w_{d,n}$ is drawn based on a $\beta$ that connects that topic to a particular set of members in the vocabulary, where a particular $\beta$ can be influenced by the conent covariate of that document, as well as interactions betweeen that covariate leve and the given topic.


the full posterior of interest is $p(\mathbf {\eta, z,\kappa, \gamma, \Sigma} |\mathbf{w, X, Y }) \propto$

\begin{equation}
\bigg(\prod^D_{d=1}Normal(\mathbf{\eta_d}|\mathbf{X}_d\mathbf{\gamma}, \mathbf{\Sigma}) \Big(\prod^N_{n=1}Mult(z_{n,d}|\mathbf{\theta}_d) \times Mult(w_n|\mathbf{\beta}_{d,k=z_{d,n}} \Big) \bigg) \times \Pi p(\kappa) \Pi p(\mathbf{\Gamma})
\end{equation}

$\theta_d$ is the vector of topic proportions, and the prior for $z_{d,n}$

$z_{d,n} $ is latent variable for the nth word in the $d$-th document? (and the prior for $w_{d,n}$? connection to the topics?)

$w_{d,n}$ is the observed occurance of the $n$-th word slot in the $d$-th document


\subsection{Estimation}

The posterior given above is intractable to any closed form evaluation. \citet{stm2016} utilize an approximate variational Expectations Maximization (EM) developed by \citet{WangBlei2013}.

find the approximate posterior $\Pi_d q(\mathbf{\eta}_d)q(\mathbf{z}_d)$ where $q(\mathbf{\eta}_d)$ is a Gaussian with mean $\lambda_d$ and covariance $v_d$ and $q(\mathbf{z}_d)$ is a multinomial with parameter $\phi_d$

in E step, iterate through each document, updating variational posteriors $q(\mathbf{\eta}_d)$ and $q(\mathbf{z}_d)$

in M step, maximize the the approximate "Evidence Lower Bound" with respect to $\mathbf{\Gamma}$, $\mathbf{\Sigma}$, and $\mathbf{\kappa}$



\section{Application}

\subsection{Data: American National Election Studies}

The American National Election Studies has been collecting survey data on elections in the United States since 1948. In addition to numerous traditional survey forms, they have also continuously asked several questions which elicit open ended responses from participants. These include the "like/dislike" questions, which have been collected both on each of the two major US political parties (and their congressional candidates) during most presidential and midterm elections as well as for the parties' presidential candidates during presidential election years.  For this analysis, we focus on the "like/dislike" responces specifically for presidential candidates, which read as follows:

\begin{quote}
Now I'd like to ask you about the good and bad points of the two candidates for President. Is there anything in particular about [Democratic presidential candidate]/[Republican presidential candidate] that might make you want to vote \textit{for} him?  What is that? Anything else?
\end{quote}

\begin{quote}
Is there anything in particular about [Democratic presidential candidate]/[Republican presidential candidate] that might make you want to vote \textit{against} him? What is that?  Anything else?
\end{quote}

While the ANES has collected open ended responses to these questions over many years, little has been done with them due to the high cost of analysis referenced above. We apply the Structural Topic Model to a subset of these open ended surveys, collected during presidential election years, from Ronald Reagan's reelection campaign of 1984 to George W. Bush's reelection campaign of 2004. Although the ANES also collected similar responses on the independent candidate, Ross Perrott, in 1992 and 1996, we have omitted these data from the analysis to be consistent across all six years. In addition to the open ended survey responses, the ANES also collects basic demographic information on each respondent. As potential prevalence covariates, we explore the year the responses were collected, as well as the respondents' race, gender, political affiliation, church attendance, educational attainment, income percentile, and ideology.

\subsection{Text Pre-Processing}


To prepare the open ended survey respones for analysis, we first applied some basic text preprocessing. Although the STM package includes some functions from the text mining (tm) package, we opted to use the quanteda package, since it offered a bit more flexibility and its output could easily be converted into an object which the stm package could utilize. This process involved several steps:
\begin{enumerate}
\item remove punctruation
\item stemming
\item partial 2-grammming
\item stop word removal (ie, removal of generally very high frequency features)
\item very low frequency feature removal
\end{enumerate}

Stemming ensures that words like "immigrant" and "immigrants" and "immigration" are recognized as a single feature within the corpus, rather than three separate features. "Partial 2-gramming" retains most features in the vocabulary as single words (ie 1-grams), but belnds certain phrases of two word length into single features, so that, for example, "vice president" or "social security" are recognized as single features, rather than two separate features, and thus are "seen" by our model as a different features than "president" or "security" respectively. We opted for "partial 2-gramming" rather than using 2-grams of the entire corpus, since models using just 1-grams or all 1- and 2-grams generated fairly similar models, but topic estimation for models using all possible 1- and 2-grams took significantly longer amounts of time to run. Stop word removal involves filtering out a standard set of English "function" words, including most articles and prepositions, as well as some corpus specific features, like "ae" which indicates that the interviewer asked "Anything else?" Lastly, very low frequency words ($n \leq 3$ instances) which might slow computation but add nothing to the interpretability of the model are also removed. 

Two example responses are given below, showing first the unprocessed response followed by a "toy" document feature matrix representing a corpus of only that individual document. Document feature matrices representing corpora with more than one document will almost always have some entries which are zero, since two documents will rarely have exactly the same set of words represented. 

Example 1: Original text
<<ex1orig, eval=TRUE, echo=FALSE>>=
print(str_break(example1))
@

Example 1: Preprocessed text
<<ex1prepro, eval=TRUE, echo=FALSE>>=
rbind(example2@Dimnames$features, example2@x)
@

Example 2: Original text
<<ex2orig, eval=TRUE, echo=FALSE>>=
print(str_break(example3))
@

Example 2: Preprocessed text
<<ex2prepro, eval=TRUE, echo=FALSE>>=
rbind(example4@Dimnames$features, example4@x)
@

After this preprocessing of the corpus, the remaining dataset contains 23,507 instances of open ended responses with counts for 3,357 features (1-grams and selected 2-grams). This corresponds to 9,977 individuals who responded to between one and four of these questions over the course of six presidential elections, taking place every four years between 1984 and 2004.

%% this is incorrect. need to fix it After this process, the corpus is then represented by a 23,507 rows by 3,357 column document feature matrix, denoted $ \mathcal{X} $, in which the $d$-th row corresponds to the $d$-th document and the $w$-th column represents the $w$-th feature in the vocabulary. Then the $d,w$-th entry in the matrix represents the number of times the $w$-th feature occurs in the $d$-th document. This matrix is very sparse, with the vast majority of entries being zeros.

\subsection{Model}

After fitting models with various numbers of topics, I settled on models using $K=60$ topics. This is close to the number of topics ($K=69$) which Roberts et al (cite) fit for their analysis of a open ended survey data from 2008 inquiring about the top issues of the campaign, for which they were able to obtain and compare hand coded analysis.

As a baseline model, I fit just the election year as a covariate. Rather than fitting this as a linear coefficient, I opted for a categorical variable to avoid forcing the prevalence of a given topic to either strictly increase or strictly decrease over time. This model reduces similarly to the correlated topic model presented in \citet{Blei:2007jy}

A second baseline model incorporated the party identification of the respondent, as well as interactions between this covariate and the year. 

In addition to the year and respondent's party ID, further models fit one of the selected covariates (respondent's race, gender, income percentile, educational attainment, church attendence, ideology, or feeling towards the candidate ), 

While our intention had been to examine how topics and topic proportions differ when models with different covariates are fit, none of the models fit appear to be particularly different.

Comparison of word clouds and scree plots suggest only very minor differeces both in overall word proportions and FREX calculations. None of the 10 models fit display enough of a difference to visually justify that they are particularly different from one another. These differences (or overall lack thereof, are discussed in the following section).



\section{Vizualizations}

\jp{still more cleaning needed, plus need to pick some plots. i will include all plots of each type in the appendix, but i'm not sure how to decide which plots i should include in this section}

Comparison of these models was not straight forward. Without some knowledge of distributions to compare to, goodness of fit or other such diagnostics are hard to come by. However, there is still a great deal of exploratory analysis which can be performed, particularly via visualiaztion. Incorporation of expert (political science) opinion by way of examination of wordclouds, proportion plots, and exemplar documents can also provide valuable insight with regard to which topics do or do not contain useful and coherent semantic information. These are discussed in turn, accompanied by figures for a selected number of more interesting topics. Plots for all topics are contained in the Appendix.

At this point, there isn't enough variation from one model to the next to say with any certainty that our identified covariates have any identifiable effects on the estimated topics, how much those topics show up within a given document, or for a given group of respondents.

Perhaps the most fun and also the most frustrating part of this project was considering ways to summarize the information gleaned from fitting the various models. While it is our hope that as scholars continue to use and explore these procedures, more easily assessible metrics of fit will be developed, we focus mostly on graphical examiniations.

Most of the plotting done throughout this papwer leverages the amazing power and flexiblity of ggplot and other packages within tidyverse to make plots more dynamic and informative, something which is much harder to do efficiently when relying on the base R plotting functionality, as is the case with the STM package.

\subsection{Simultaneous plotting of multiple covariates}

For a given topic, the prevalence of that topic can vary by any covariate incorporated into the model. In the models fit, topics vary above or below the mean for that topic by year, party ID, and other covariates as they're specified. Interactions between covariates are also allowed.


%%add some plots from partyQ model, some like/dislike, maybe one or two from race or gender?

\subsection{Wordclouds}
To make the wordclouds more informative, two measures were mapped to the displays. Firstly, raw proportions were mapped to the size of the word. However, as noted in Roberts and elsewhere, high frequency words alone may obscure some of what is going on within a given topic if those words also occur frequently in other topics. Thus, their FREX calculation, which also incorporates a weighted measure of topic "exclusivity" is mapped to the saturation of a word, so that words which also occur elsewhere show up darker (in black), and words which are more exclusive to the given topic show up more brightly (in this case, in red).

Wordclouds are generated using ggplot. For a given topic, we identify the highest frequency words, and display them in the plot, proportion mapped to size such that larger words imply a higher presence within that topic. However, as \citet{stm2013} and \citet{FREX2012} note, high frequency words alone may fail to capture the "essense" of the topic, since high frequency words may also appear in other topics. To address this, \citet{FREX2012} developed a Frequency-Exclusivity metrix, "FREX", which can combine a particular word's frequncy within a given topic and its exclusivity towards other topics. The FREX value for the $v$-th word and the $k$-th topic is computed by:

\begin{equation}
FREX_{k,v} = \Big(\frac{w}{ECDF(\beta_{k,v}/\sum^K_{j=1}\beta_{j,v})} + \frac{1-w}{ECDF(\beta_{k,v})} \Big)^{-1}
\end{equation}

where $w$ is a value chosen by the analyst to distribute weight between Exclusivity (left side addend) and Frequency (right addend) (we set it to $w=0.5$), and ECDF is the empirical cumulative distritubion function, and $\beta_{k,v}$ indicates the probability of the $v$-th term given the $k$-th topic. 

To make our wordclouds more informative, we compute the FREX value for the entire vocabularly, conditioned on the topic, and then map those values to a two color gradient, so that the more exclusive a word is, the more red it will be in the graphic. Thus a term that is both frequent and exlusive will show up large and red, where as a term that is frequent but less exlusive will be a less vibrant shade of red, or even gray or black. Likewise, a term that is less frequent but still exclusive will be smaller, but can still be a brighter red. By combining both the overall prevalence of words within a topic and their FREX values, we present a more cohesive picture of each topic than either metric alone.

Additional wordclouds are created to inspect the ways in which one of two content covariates influence word generation within a topic. We considered two models, one with Voter sentiment (like/dislike) assigned as the content covariate, and the other with Candidate Party (Democrat/Republican) mapped to the content covariate, to see if either of these influnced \textit{how} respondents discussed topics. 

Here again, we map the proportion with which a word shows up in a given topic to the size of the term in the plot. But for a given term, now we map how much the term's frequency, conditioned on selected topic, content covariate level, and their interactions, deviates from the prevalance of that term across the corpus to a three color gradient, with gray being the central color. Terms that are not particularly biased "towards" either level of the content covariate are mapped towards the center of the gradient, whereas terms that are more likely for one level or the other are mapped to the corresponding color.

At the suggestion of \citet{stm2016}, while in theory we could assign more complicated covariates to to the content covariate of the model, going from no content covariate to a categorical content covariate with two levels doubles the number of paraemeters of the model. Two content covarites each with two factor levels would quadruple the numbe of parameters, and so forth. Thus, as we explored the STM, in the models we tried which incorporated content covariates, we opted for covariates that only had at most two factor levels which exhaustively caputred all documents. 

%%plotting of content covariate by color allows indication of words used more for(purple)/againast(green) and when describing the dem(blue)/rep(red) candidate

%%a few sets of three wordclouds - basic/frex, dem vs rep candidate, and like vs dislike

\subsection{Scree Plots}

Contributions of topics to the coprus

largest contributions of topics to groups of documents grouped by a levels of a covariate

Contribution of indiviual words to given topics
%%top 40 words for each topic, where height indicates how much the word shows up (unnormalized) and red hue indicates FREX calc as noted in the previous subsection


\section{Discussion and conclusions}

\jp{i need to organize these stray thoughts into something more coherent}

The flexibility of STM is both a blessing and a curse. Even with a deeper knowledge of stats, examining/understanding/interpreting models is not entirely straight forward. If the goal is to make available for social sciences, humanities, many may lack the background to apply the model in a reliable way, particularly if it has any complexity.

need more in the way of model fit stats, or some kind of nested model comparison?

application to larger texts

more guidance on how selecting K impacts the models - some way to meausure if there's too many or two few topics for a corpus - ie too many "garbage" topics

ways to compare between various K sized models

seems odd that adding covariates didn't really change the topic estimation if topic estimation happens at same time as param estimation - maybe too many covariates or levels of corvatiates?

cleaner/more efficient way to reparameterize?

my functions to fill in some of the gaps in the STM package's functions, better and more varied plotting, tidying param values to facilitate plotting


\section{What next}

\jp{future work, things i wish i could have considered, spent more time on}

Compare expert political science opinion to large number of lay opinions. present topics (or exemplars) to large number of people and see what they think the topics about via online tools like amazon's mechanical turk

there's a lot more responses for the midterms. curious how those responses and models might be different. several of the presidental model topics are person/year specific (particular to bush or clinton or gore or reagan, etc). probably wouldn't see so much of that for the congressional elections.

is there a way to parallelize the EM process ?

more diagnostics, especially if adding additional covariates actually makes the model better 

\bibliographystyle{asa}

\bibliography{references}


\end{document}


\hh{you'll need to define every symbol and index that you use.}

$\beta_{d,v}^k$ is the sum of the covariate effects?, and a prior for $w_{d,n}$

\begin{centering} 
Topic Prevalence:
\begin{align*}
	\mu_{d,k} &= X_d\gamma_k \\
	\gamma_k &\sim \mathcal{N}(0,\sigma_k^2)\\
	\sigma_k^2 &\sim Gamma(s^{\gamma}, r^{\gamma})
\end{align*}

$\mu_{d,k}$ is the expected topic prevalence of the k-th topic in the d-th document 

$\gamma_k$ is the prior for the kth topic mean

$\sigma^2_k$ is the prior for the kth topic variance

Language Model:
\begin{align*}
\theta_{d} &\sim LogisticNormal(\mu_d, \Sigma) \\
z_{d,n} &\sim Mult(\theta_d)\\
w_{d,n} &\sim Mult(\beta^{k=z_d,n}_d, r^{\gamma})
\end{align*}

\end{centering}

\centering Topical Content:
\begin{align*}
\beta_{d,v}^k &\propto exp(m_v+\kappa_v^{.,k}+\kappa_v^{y,.}+\kappa_v^{y,k}) \\
\kappa_v^{y,k} &\sim Laplace(0,\tau_v^{y,k})\\
\tau_v^{y,k} &\sim Gamma(s^{\kappa}, r^{\kappa})
\end{align*}