\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\title{A tbd title for STM paper}
\author{Joseph Eduardo Papio}

\begin{document}


\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE)

library(tm)
library(ngram)
library(RWeka)
library(quanteda)
library(RColorBrewer)
library(stm)
library(stringi)
library(Rtsne)
library(geometry)

set.seed(11123)

dfTranscripts <- read.table("transcriptsB.csv",header=F,sep="\t",colClasses=c("character","character"), col.names=c("url","text"),quote="")
@

\section{Introduction and Literature Review}
 

Methods of automated textual analysis 
Topic models-Application of Latent Dirichlet Allocation to the process of automatic/computer aided textual analysis known as Topic Models. 

To facilitate analysis, surveys are often collected in such a way as to give respondents a choice from one of several preselected options, since analysis of open ended survey responses can be very time and labor intensive, requiring human coders to read through the indvidual responses, make judgements, and then code what thematic information is contained within them. 

Topic modeling is an attempt to bring some reliable level of automatiion to qunatifying the thematic information in the text responses collected from open ended surveys. We define each response as a document and the specific set of survey responses as our corpus. To determine the thematic content of the documents and the corpus,  with the goal of estimating the proportions with which those themes, which we define as topics, occur within the documents of the corpus. %(loosely topics are themes, and more technically, they are non exclusive clusters of words. distance between/within clusters is (?) computed based on the frequencies of word (co?)occurance within/across documents ) 

 "Simple" Latent Dirichlet Allocation is the base form of this method, built on the common assumptions of exchangeability and independence. %exchangability - conditional independence, ith and jth instances can be switched, order doesn't matter
 Obviously, the order of the words as they appear does matter for English syntax, but LDA treats each document as a "bag of words", essentially ignoring syntactic information and focusing entirely on the semantic information.
 Unsupervised classification problem in machine learning (topics are not known before hand)
 %\hh{define exchangeability and independence in the context of LDA}

%\hh{what data are LDA models dealing with? - give an example. the xkcd data would be good for the intro. }
%\hh{what is the goal of an LDA analysis? }

Unlike previous probabilistic methods to model documents, such as probabilistic latent semantic indexing (hoffman 99), LDA is a (vast/stark?) improvement because it can assign probabilities beyond the documents included when the model was fit to documents not originally included. pLSI also requires increasing the number of parameters as the number of documents within a corpus increases.
%\hh{ give an example (with citation) of a previous method}

A major limitation of the original LDA is its inability to incorporate correlation across topics. 
In practice, independence between documents is an unrealistic expectation when dealing with large sets of documents. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that came before them. 
Correlated topic models build on LDA by relaxing the independence assumption (cite blei/lafferty 2007) \hh{give the math of how this independence assumption is relaxed.} 

Structural topic models further build on the additions of correlated topic models by incorporating document level covariates, such as particular characteristics of the author(s). (cite roberts et al 2013) \hh{please give an example here - it's getting very abstract.}

For example, in the ANES dataset, document level covariates include the respondent's gender, race, and/or party affiliation.

\citet{stm2013} apply their method of Structural Topic Modeling %\hh{what is their method?} 
to open ended survey data from the ANES, collected in 2008. We futher apply their method to a larger amount of data from the ANES, open ended survey questions collected every presidential election year from 1948 through 2004. In addition to the open ended survey responses, basic demographic information was also collected, including the respondents' race, gender and political affiliation.

% with LDA, priors for topics and words are dirichlets, whereas with STM, priors for toopics are Normal-Gamma topic and Gamma Lasso for kappa 


\section{Methods}
apply LDA (similar to xkcd) to ANES data without taking respondent information into account
then apply STM to ANES data, taking respondent information into account and compare

\section{Applications}

\subsection{xkcd}

The xkcd corpus consists of transcriptions of the first 1265 comics and artworks produced at xkcd.com. To act as a meta-document covariate, a dummy variable "group" was added, dividing the corpus into 11 groups of 115 documents by simply taking the first 115 documents and calling them "group 1", the second 115 documents "group 2", and so forth.

Prior to fitting models to the open ended text corpus, the documents must first undergo some basic text preprocessing. For the xkcd dataset, documents begin as one or more sentences plus some descriptive text, a transcription:

<<examples1, echo=FALSE>>=
xTran <- Corpus(VectorSource(dfTranscripts$text))

#inspect a few documents in the corpus
print("Comic 30")
writeLines(as.character(xTran[[30]]))
print("Comic 612")
writeLines(as.character(xTran[[612]]))

#store to demonstrate chnages as text processing progresses
ex1a <-as.character(xTran[[30]])
ex2a <-as.character(xTran[[612]])

#convert to lower case
xTran <- tm_map(xTran, content_transformer(tolower))

ex1b <-as.character(xTran[[30]])
ex2b <-as.character(xTran[[612]])

#remove stopwords, including "alt-title", which is in most of the comics as the mouse over text
xTran <- tm_map(xTran, removeWords, c(stopwords("english"), "alt-title", "alt", "title"))

ex1c <-as.character(xTran[[30]])
ex2c <-as.character(xTran[[612]])

#remove punctuation and numbers
xTran <- tm_map(xTran, removePunctuation)
xTran <- tm_map(xTran, removeNumbers)

ex1d <-as.character(xTran[[30]])
ex2d <-as.character(xTran[[612]])

#stemming
xTran <- tm_map(xTran, stemDocument)

ex1e <-as.character(xTran[[30]])
ex2e <-as.character(xTran[[612]])

#remove any unnecessary white space
xTran <- tm_map(xTran, stripWhitespace)

ex1f <-as.character(xTran[[30]])
ex2f <-as.character(xTran[[612]])


@

Then, the text is converted to lower case, the numbers and punctuation are removed, English stop words are removed, and the words are stemmed so that different forms of base words, like singular and plural instances, or different tenses, are counted as the same word. Lastly, any additional spacing between words is removed, resulting in a string of base forms for each document:

<<examples2, echo=FALSE>>=
ex1f
ex2f
@

After this, the corpus is converted into a document feature matrix consisting of the 1-gram and 2-gram tokens for each document. The \textit{d}-th row of this matrix represents the \textit{d}-th document, and the \textit{w}-th column represents the \textit{w}-th n-gram in the vocabulary. Then the \textit{d,w}-th entry in the matrix is the number of times the \textit{w}-th n-gram occurs in the \textit{d}-th document. During this process, highly common n-grams which occur in all or almost all the documents are trimmed, as are highly uncommon n-grams which occur in only a very few number of documents. This cuts down on computation time with minimal loss of information, since very high frequency words can tell us little about what distinguishes documents in order to estimate topics and their prevalence, while very low frequency words can tell us little about what topics documents have in common.

The resultant document feature matrix for the xkcd data set consists of 1265 rows (documents) and () columns (n-grams).

The most common ngrams are (add later)



\subsection{ANES}

\section{Conclusion}

could also reference dynamic topic models (blei, lafferty 2006) which adds a temporal  element to the model, since data are collected over time. \hh{are you planning to model the temporal aspect? If not, this would be better in the conclusions and future work statement.}


\hh{you'll need to define every symbol and index that you use.}

\centering Topic Prevalence:
\begin{align*}
	\mu_{d,k} &= X_d\gamma_k \\
	\gamma_k &\sim \mathcal{N}(0,\sigma_k^2)\\
	\sigma_k^2 &\sim Gamma(s^{\gamma}, r^{\gamma})
\end{align*}

\centering Language Model:
\begin{align*}
\theta_{d} &= LogisticNormal(\mu_d, \Sigma) \\
z_{d,n} &\sim Mult(\theta_d)\\
w_{d,n} &\sim Mult(\beta^{k=z_d,n}_d, r^{\gamma})
\end{align*}

\centering Topical Content:
\begin{align*}
\beta_{d,v}^k &\propto exp(m_v+\kappa_v^{.,k}+\kappa_v^{y,.}+\kappa_v^{y,k}) \\
\kappa_v^{y,k} &\sim Laplace(0,\tau_v^{y,k})\\
\tau_v^{y,k} &\sim Gamma(s^{\kappa}, r^{\kappa})
\end{align*}






\bibliographystyle{asa}
\bibliography{references}


\end{document}
