\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\title{A tbd title for STM paper}
\author{Joseph Eduardo Papio}

\begin{document}


\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE, cache=TRUE, width=40)

library(tm)
library(ngram)
#library(RWeka)
library(quanteda)
library(RColorBrewer)
library(stm)
library(stringi)
library(Rtsne)
library(geometry)

set.seed(11123)

dfTranscripts <- read.table("transcriptsB.csv",header=F,sep="\t",colClasses=c("character","character"), col.names=c("url","text"),quote="")

str_break = function(x, width = 70L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}
@

\section{Introduction and Literature Review}
 
Open ended survey responses provide an opportunity to solicit unguided opinions and ideas from participants, avoiding possible biases that can be introduced when utilizing more traditional survey forms. More traditional survey forms, on the other hand, require participants to select from several predetermined options in a closed set of choices. 

The immediate benefit of the traditional survey forms is that once the data are collected, analysis, including estimation of the prevalence of those opinions within the population, is fairly straight forward, and generally requires only relatiely low amounts of personnel and computing time. 

Until the last decade or so, little existed in the way of automated processing or analysis of open ended surveys. Identification and analysis of the content of the responses often requires a large number of person-hours, in which trianed readers go through individual responses, using their judgement to identify the presence or absence of one or more topical areas from a predetermined list. Thus, in addition to the large resource requirements, some topical information contained in the responses might still be lost when utilizing this method of analysis if that topical information didn’t fit neatly into one of those predetermined areas. %Which is to say, the application of of this processing via human readers might introduce bias into the results through the predetermined list of topic options.


%Enter topic models, pLSI (Hoffman) then LDA (Blei), then STM
Topic models attempt to bring some reliable level of automation to quantifying the thematic information present in the text responses collected through open ended surveys. We treat each response as a document, which we define as the i-th instance (exemplar?) within the corpus. We define the corpus as the entire collection of documents (responses) of interest. 

Topics, then, exist within the corpus, as groups of words which have high co-occurrence within documents. Words are not exclusive to a particular topic, but rather may occur in multiple topics. Once topics are computed, then the proportion of each topic within each document can be estimated.

 %(loosely topics are themes, and more technically, they are non exclusive clusters of words. distance between/within clusters is (?) computed based on the frequencies of word (co?)occurance within/across documents ) 

(discussion of pLSI here) \citep{hofmann:1999} % pLSI requires increasing the number of parameters as the number of documents within a corpus increases, cannot assign probabilities beyond the dataset provided to fit the model, so can’t apply things like test/training set hold outs to test model, etc?

Latent Dirichlet Allocation (LDA) \cite{blei2003} builds on the work of pLSI, allowing for the assignment of probability beyond the documents included when the model was fit to documents not originally included. It also avoids the increase in the number of estimated parameters as the number of documents increases. Blei, Ng and  Jordan treat each document as a bag of words in which the order of the words is not taken into account. Although this causes some loss of syntactic information, the semantic content of the documents is what is of interest here. And what is gained by this bag of words approach is allowing exchangabilty and independence in .. (?)   %\hh{define exchangeability and independence in the context of LDA}

Blei futher builds on LDA, addressing the somewhat unrealistic expectation in large collections of documents that there is no correlation among topics or documents, with the introduction of Correlated Topic models \citep{Blei:2007jy}. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that were published previously. Thus, correlated topic models build on LDA by relaxing the independence assumption.
%\hh{give the math of how this independence assumption is relaxed.}

Structural Topic Models (STM) \cite{stm2013} further builds on this work by allowing for the inclusion of document level covariates. In the open ended surveys we analyze, such covariates include demographic information about the respondent, such as race, gender, age, and/or party affiliation.
%\hh{please give an example here - it's getting very abstract.}

\citet{stm2013} apply their method of Structural Topic Modeling %\hh{what is their method?} 
to open ended survey data from the ANES, collected in 2008. We futher apply their method to a larger amount of data from the ANES, open ended survey questions collected every presidential election year from 1948 through 2004. In addition to the open ended survey responses, basic demographic information was also collected, including the respondents' race, gender and political affiliation.


% with LDA, priors for topics and words are dirichlets, whereas with STM, priors for toopics are Normal-Gamma topic and Gamma Lasso for kappa 


\section{Methods}
apply STM to xkcd dataset, using fake "group" variable 
then apply STM to ANES data, taking respondent information into account and compare

\section{Applications}

\subsection{xkcd}

The xkcd corpus consists of transcriptions of the first 1265 comics and artworks produced at xkcd.com. To act as a meta-document covariate, a dummy variable "group" was added, dividing the corpus into 11 groups of 115 documents each by simply taking the first 115 documents and calling them "group 1", the second 115 documents "group 2", and so forth. This variable loosely conveys some aspects of time, since the comics are arranged in the order in which they were published. %\hh{this group variable will capture some time component, right?}

Prior to fitting models to the open ended text corpus, the documents must first undergo some basic text preprocessing. For the xkcd dataset, documents begin as one or more sentences plus some descriptive text, a transcription:

<<examples1, echo=FALSE, tidy=TRUE>>=
xTran <- Corpus(VectorSource(dfTranscripts$text))

#inspect a few documents in the corpus
cat("Comic 30:")
print(str_break(as.character(xTran[[30]])))
cat("Comic 612:")
print(str_break(as.character(xTran[[612]])))

#store to demonstrate chnages as text processing progresses
ex1a <-as.character(xTran[[30]])
ex2a <-as.character(xTran[[612]])

#convert to lower case
xTran <- tm_map(xTran, content_transformer(tolower))

ex1b <-as.character(xTran[[30]])
ex2b <-as.character(xTran[[612]])

#remove stopwords, including "alt-title", which is in most of the comics as the mouse over text
xTran <- tm_map(xTran, removeWords, c(stopwords("english"), "alt-title", "alt", "title"))

ex1c <-as.character(xTran[[30]])
ex2c <-as.character(xTran[[612]])

#remove punctuation and numbers
xTran <- tm_map(xTran, removePunctuation)
xTran <- tm_map(xTran, removeNumbers)

ex1d <-as.character(xTran[[30]])
ex2d <-as.character(xTran[[612]])

#stemming
xTran <- tm_map(xTran, stemDocument)

ex1e <-as.character(xTran[[30]])
ex2e <-as.character(xTran[[612]])

#remove any unnecessary white space
xTran <- tm_map(xTran, stripWhitespace)

ex1f <-as.character(xTran[[30]])
ex2f <-as.character(xTran[[612]])


@

Then, the text is converted to lower case, the numbers and punctuation are removed, English stop words are removed, and the words are stemmed so that different forms of base words, like singular and plural instances, or different tenses, are counted as the same word. Lastly, any additional spacing between words is removed, resulting in a string of base forms for each document:

<<print2, echo=FALSE, warning=FALSE, message=FALSE,  verbose=FALSE>>=
cat("Comic 30:")
print(str_break(ex1f))
cat("Comic 612:")
print(str_break(ex2f))
@

<<examples2, echo=FALSE, message=FALSE, verbose=FALSE, results='hide'>>=
#convert vector from dataframe into an corpus for quanteda package
myCorpus <- corpus(dfTranscripts$text)
docvars(myCorpus, "group") <- as.factor(rep(1:11,each=115))

#create document feature matrix, with 1-grams and 2-grams
mydfm <- dfm(myCorpus, stem=TRUE, ngrams=c(1,2), 
             removeNumbers=TRUE, removePunct=TRUE,
             removeSeparators=TRUE, ignoredFeatures=c(stopwords("english"), "title", "text"))

#remove words with less than 4 occurances and in only one document
mydfm <- trim(mydfm, minCount=4, minDoc=2)

#further trim to include 5 or more occurances and in at least two docs
mydfm <- trim(mydfm, minCount=5, minDoc=3)

stmdfm <- convert(mydfm, to="stm",
                  docvars=data.frame(group=docvars(myCorpus, "group")))

stmFit <- stm(stmdfm$documents, stmdfm$vocab, K=10, prevalence= ~group, data=stmdfm$meta, init.type="Spectral" )

@

After this, the corpus is converted into a document feature matrix consisting of the 1-gram and 2-gram tokens for each document. The \textit{d}-th row of this matrix represents the \textit{d}-th document, and the \textit{w}-th column represents the \textit{w}-th n-gram in the vocabulary. Then the \textit{d,w}-th entry in the matrix is the number of times the \textit{w}-th n-gram occurs in the \textit{d}-th document. During this process, highly common n-grams which occur in all or almost all the documents are trimmed, as are highly uncommon n-grams which occur in only a very few number of documents. This cuts down on computation time with minimal loss of useful information, since very high frequency words can tell us little about what distinguishes documents in order to estimate topics and their prevalence, while very low frequency words can tell us little about what topics documents have in common.

The resultant document feature matrix for the xkcd data set consists of 1265 rows, one for each document, and 2850 columns, one for each non trimmed n-gram remaining in the vocabulary. 

<<examples3, echo=FALSE>>=
as.matrix(mydfm)[c(30,612),40:50]

#Plot wordcloud
#plot(mydfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))


@

From here, the STM package (or LDA package) can be applied to the data, and topics and their proportions in the documents and the corpus can be estimated and displayed. 

\begin{figure}
<<examples4, echo=FALSE>>=
plot.STM(stmFit, type = "summary", xlim = c(0, .3))
@
\caption{\label{stm-plot} \hh{Plot result from plot.STM - this needs to have a better caption}}
\end{figure}

\subsection{ANES}

\section{Conclusion}

could also reference dynamic topic models (blei, lafferty 2006) which adds a temporal  element to the model, since data are collected over time. \hh{are you planning to model the temporal aspect? If not, this would be better in the conclusions and future work statement.}


\hh{you'll need to define every symbol and index that you use.}

\centering Topic Prevalence:
\begin{align*}
	\mu_{d,k} &= X_d\gamma_k \\
	\gamma_k &\sim \mathcal{N}(0,\sigma_k^2)\\
	\sigma_k^2 &\sim Gamma(s^{\gamma}, r^{\gamma})
\end{align*}

\centering Language Model:
\begin{align*}
\theta_{d} &= LogisticNormal(\mu_d, \Sigma) \\
z_{d,n} &\sim Mult(\theta_d)\\
w_{d,n} &\sim Mult(\beta^{k=z_d,n}_d, r^{\gamma})
\end{align*}

\centering Topical Content:
\begin{align*}
\beta_{d,v}^k &\propto exp(m_v+\kappa_v^{.,k}+\kappa_v^{y,.}+\kappa_v^{y,k}) \\
\kappa_v^{y,k} &\sim Laplace(0,\tau_v^{y,k})\\
\tau_v^{y,k} &\sim Gamma(s^{\kappa}, r^{\kappa})
\end{align*}






\bibliographystyle{asa}
\bibliography{references}


\end{document}
