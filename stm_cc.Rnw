\documentclass{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{pdfpages, natbib}
\usepackage{float}
\usepackage{amsmath}

\newcommand{\hh}[1]{{\color{magenta} #1}} 

\newcommand{\jp}[1]{{\color{blue}#1}}

\title{Application and Exploration of the Structural Topic Model to the American National Election Studies' Open ended Survey Responses Regarding Major Party Presidential Candidates from 1984 to 2004}

\author{Joseph Eduardo Papio}

\begin{document}

\maketitle

<<concordance, echo=FALSE, warning=FALSE, message=FALSE>>=
opts_knit$set(self.contained=FALSE, cache=TRUE, width=40)

library(tm)
library(quanteda)
library(RColorBrewer)
library(stm)
library(stringi)
library(Rtsne)
library(geometry)
library(stmBrowser)
library(dplyr)
library(tidytext )
library(reshape)

library(lda)

#library(topicmodels)
#library(Matrix)

#library(textir)

#set.seed(1332)

dfTranscripts <- read.table("transcriptsB.csv",header=F,sep="\t",colClasses=c("character","character"), col.names=c("url","text"),quote="")

str_break = function(x, width = 70L) {
  n = nchar(x)
  if (n <= width) return(x)
  n1 = seq(1L, n, by = width)
  n2 = seq(width, n, by = width)
  if (n %% width != 0) n2 = c(n2, n)
  substring(x, n1, n2)
}
@

\section{Introduction, Motivation, and Literature Review}

\subsection{Open Ended versus Traditional Surveys}
 
Open ended survey questions provide an opportunity to solicit unguided opinions and ideas from participants, avoiding possible biases that can be introduced when utilizing more traditional survey forms in which respondents are prompted to select their answer from one of several predetermined options in a closed set of choices. 

The immediate benefit of such traditional survey formats is that once the data are collected, analysis, including estimation of the prevalence of those opinions within the population, is fairly straight forward, and generally requires only relatiely low amounts of personnel power and computational resources. However, by prompting respondents to select from among these predetermined choices, which must be decided upon by researchers prior to implementations, ideas which are already present in the political collective consciousness but not yet salient to researchers may be papered over and thus lost. Considering such information loss, exploration and application of of open ended (and thus "unprimed") survey responses is an area of great interest in the humanities and social sciences.

However, because of the intense resource requirements needed for such analysis, little work has been done in the way of leveraging these open ended sources, despite a plethora of collected and available data. This is because identification and analysis of the content of these data often requires massive amounts of person-hours, in which trianed readers go through individual responses, using their judgement to identify the presence or absence of one or more topical areas from a predetermined list. Thus, in addition to the large resource requirements, some topical information contained in the responses might still be lost when utilizing this method of analysis if that topical information didnâ€™t fit neatly into one of those areas predetermined by researchers and their trained readers.

However, as computing power increases, interest in the development, exploration and application of automated textual analysis methods such as topic models and the like will surely continue to grow, especially for humanities and social science researchers. Such interest is likely to be stronger with researchers who are required to first build survey instruments before interacting with respondents, as the time required to construct the instruments prior to deployment can introduce significant lag. During such time, nascent ideas forming within the population may be missed or detected by other researchers.

\subsection{pLSI, LDA, and STM}

Topic models, a subfield of Machaine Learning and Natural Language Processing, attempt to bring some level of automation, speed, and reliability to quantifying the latent thematic information present in the text responses collected through open ended surveys. 

An early example of the efforts at to apply statistically sound methods to resonably model the latent processes that could have generated the documents in a corpus is Probabalistic Latent Semantic Indexing (pLSI) \citep{hofmann:1999}. Despite many limitations, pLSI was a pioneering step in the development of the foundation for the Topic Models framework.

Latent Dirichlet Allocation (LDA) \cite{LDA2003} built on the work of pLSI, allowing for the assignment of probability beyond the documents included when the model was fit to documents not originally included. Blei, Ng and Jordan treat each document as a "bag of words" in which the order of the words is not taken into account. Although this causes some loss of syntactic information, the semantic content of the documents is what is of primary interest in fitting Topic Models. Thus what syntactic information is lost by this bag of words approach can be neglible for our purposes.   

Blei futher builds on LDA, addressing the somewhat unrealistic expectation in large collections of documents that there is no correlation among topics or documents, with the introduction of Correlated Topic models \citep{Blei:2007jy}. For example, within the peer reviewed literature of a given field, we expect that articles written at a later date will be influenced to some degree by articles that were published previously. Thus, correlated topic models build on LDA by relaxing the independence assumption.

Structural Topic Models (STM) \cite{stm2013} further builds on this framework by allowing for the inclusion of document level covariates. In the open ended surveys analyzed below, such covariates include demographic information about the respondent, such as their race, gender, age, and/or party affiliation.

\citet{stm2013} apply their method of Structural Topic Modeling to open ended survey data from the American National Election Studies (ANES) collected in 2008 in which respondents were asked to identify the most important and second most important political problems facing the United States and personal issues in the election. 


\subsection{Words, Topics, and Documents}

We consider each response as a document, which we define as the \textit{d}-th instance within the corpus. We define the corpus as the entire collection of documents (responses) of interest. Then we define the vocabulary of that corpus to be the vector of all words or features present within the corpus. 

Topics, then, exist within the corpus, as groups of words which have high co-occurrence within documents. Topics are the latent units of meaning from which each document is generated. Words, as members of the vocabulary, are not exclusive to any particular topic, but rather occur in multiple topics with varying prevalence. Once the set of topics is computed, then the proportion of each topic within each document can be estimated.



\section{The Mathy Stuff}

\jp{I'm still need to do a lot with this part, but I wanted to push what I had to the repo so I could get feedback at least on that part.}

\begin{centering} 
Topic Prevalence:
\begin{align*}
	\mu_{d,k} &= X_d\gamma_k \\
	\gamma_k &\sim \mathcal{N}(0,\sigma_k^2)\\
	\sigma_k^2 &\sim Gamma(s^{\gamma}, r^{\gamma})
\end{align*}

$\mu_{d,k}$ is the expected topic prevalence of the k-th topic in the d-th document 

$\gamma_k$ is the prior for the kth topic mean

$\sigma^2_k$ is the prior for the kth topic variance

Language Model:
\begin{align*}
\theta_{d} &\sim LogisticNormal(\mu_d, \Sigma) \\
z_{d,n} &\sim Mult(\theta_d)\\
w_{d,n} &\sim Mult(\beta^{k=z_d,n}_d, r^{\gamma})
\end{align*}

\end{centering}

$\theta_d$ is the vector of topic proportions, and the prior for $z_{d,n}$

$z_{d,n} $ is latent variable for the nth word in the d-th document? (and the prior for $w_{d,n}$? connection to the topics?)

$w_{d,n}$ is the observed occurance of the nth word in the d-th document


Matrix of topic pravalence covariates denoted X with dimension D x P and matrix of topical content covariates denoted Y with dimension D x A


\begin{centering}

\begin{align*}
	\gamma_k &\sim Normal_P(0, \sigma^2_kI_P) \\
	\mathbf{\theta_d}&\sim LogisticNormal_{K-1}(\mathbf{\Gamma'}\mathbf{x}'_d,\mathbf{\Sigma}) \\
	\mathbf{z}_{d,n} &\sim Multinomial_K(\mathbf{\theta}_d)\\
	\mathbf{w}_{d,n} &\sim Multinomial_V(\mathbf{B z}_{d,n})\\
	\beta_{d,k,v} &= \frac{exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c) }_{y_d,v } \kappa^{(i) }_{y_d,k,v } )} {\Sigma_v exp(m_v + \kappa^{(t)}_{k,v} + \kappa^{(c) }_{y_d,v } \kappa^{(i) }_{y_d,k,v } ) } 
\end{align*}

\end{centering}

\section{Application}

\subsection{Data: American National Election Studies}

The like/dislike questions of the ANES open ended responses have been collected over many years, but due to the high cost of analysis referenced above, little has been done with these data. I apply the Structural Topic Model method to a subset of these open ended surveys, which were collected during presidential election years, from Ronald Reagan's reelection campaign of 1984 to George W. Bush's reelection campaign of 2004. For each of the two major party candidates, respondents are asked to idenitify what they liked or disliked about that candidate. Although the ANES also collected similar responses on the independent candidate, Ross Perrott, in 1992 and 1996, I have omitted these data from the analysis so the parameters fit for each year are cnosistent. In addition to the open ended survey responses, the ANES also collects basic demographic information on each respondent. As potential prevalence covariates, I explore the year the respones were collected, as well as the respondents' race, gender, political affiliation, church attendence, educational attainment, income percentile, and ideology.

\subsection{Text Pre-Processing}

\jp{I'll add some examples using a few of the exemplar texts I pulled.}

To prepare the open ended survey respones for analysis, I first applied some basic text preprocessing. Although the STM package includes some functions from the text mining (tm) package, I opted to use the quanteda package, since it offered a bit more flexibility and its output could easily be converted into an object which the stm package could utilize. This process involved removing as much punctuation as possible, stemming so that words like "immigrant" and "immigrants" and "immigration" were recognized as a single feature rather than three separate ones, partial 2-gramming so that phrases like "vice president" were treated as a single feature rather than two separate features, removal of a standard list of stop words as well as a few "words" particular to this dataset (like "ae", indicating the interviewer asked "Anything else?"), and removing very low frequency words ( $ n \leq 3 $ instances ) which might slow computation but add nothing to the interpretability of the model. 

After this preprocessing of the coprus, the remaining dataset contains 23,522 instances of open ended responses with counts for 3,357 features (one-grams and selected two-grams). This corresponds to 9,977 individuals who responded to between one and four of these questions over the course of six presidential elections, taking place every four years between 1984 and 2004.

This process yeilds a document feature matrix, denoted $ \mathcal{X} $, in which the \textit{d}-th row corresponds to the \textit{d}-th document, and the \textit{w}-th column represents the \textit{w}-th n-gram in the vocabulary. Then the \textit{d,w}-th entry in the matrix is the number of times the \textit{w}-th n-gram occurs in the \textit{d}-th document. This matrix is very sparse, with the vast majority of entries being zeros.

\subsection{Model}

After fitting models with various numbers of topics, I settled on models using $K=60$ topics. This is close to the number of topics ($K=69$) which Roberts et al (cite) fit for their analysis of a open ended survey data from 2008, for which they were able to obtain and compare hand coded analysis.

As a baseline model, I fit just the election year as a covariate. Rather than fitting this as a linear coefficient, I opted for a categorical variable to avoid forcing the prevalence of a given topic to either strictly increase or strictly decrease over time.

A second baseline model incorporated the party identification of the respondent, as well as interactions between this covariate and the year. 

In addition to the year and respondent's party ID, further models fit one of the selected covariates (respondent's race, gender, income percentile, educational attainment, church attendence, ideology, or feeling towards the candidate ), 


\section{Vizualizations}

\jp{this part is still pretty rough}

Comparison of these models was not straight forward. Without distributions to compare to, goodness of fit or other such diagnostics are hard to come by. However, there is still a great deal of exploratory analysis which can be performed, particularly via visualiaztion. Incorporation of expert (political science) opinion by way of examination of wordclouds, proportion plots, and exemplar documents can also provide valuable insight with regard to which topics do or do not contain useful and coherent semantic information. These are discussed in turn, followed by figures for a selected number of more interesting topics. Plots for all topics are contained in the Appendix.

\subsection{Simultaneous plotting of multiple covariates}

For a given topic, the prevalence of that topic can vary by any covariate incorporated into the model. In the models fit, topics vary above or below the mean for that topic by year, party ID, and other covariates as they're specified. Interactions between covariates are also allowed.


\subsection{Wordclouds}
To make the wordclouds more informative, two measures were mapped to the displays. Firstly, raw proportions were mapped to the size of the word. However, as noted in Roberts and elsewhere, high frequency words alone may obscure some of what is going on within a given topic if those words also occur frequently in other topics. Thus, their FREX calculation, which also incorporates a weighted measure of topic "exclusivity" is mapped to the saturation of a word, so that words which also occur elsewhere show up darker (in black), and words which are more exclusive to the given topic show up more brightly (in this case, in red).

\subsection{Scree (?) Plots}

Contribution/presence of indiviual words to/in given topics by decreasing prevalence 

\section{Discussion and or conclusions}




\bibliographystyle{asa}

\bibliography{references}


\end{document}













\hh{you'll need to define every symbol and index that you use.}




\centering Topical Content:
\begin{align*}
\beta_{d,v}^k &\propto exp(m_v+\kappa_v^{.,k}+\kappa_v^{y,.}+\kappa_v^{y,k}) \\
\kappa_v^{y,k} &\sim Laplace(0,\tau_v^{y,k})\\
\tau_v^{y,k} &\sim Gamma(s^{\kappa}, r^{\kappa})
\end{align*}


$\beta_{d,v}^k$ is the sum of the covariate effects?, and a prior for $w_{d,n}$


slightly different set up from Roberts, Stewart, Airoldi 
%I think they're basically the same but just slightly different notation. need to match them up
% http://scholar.princeton.edu/files/bstewart/files/stm.pdf



